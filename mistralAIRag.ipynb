{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMW/rdzpRkKRJu0w4QAqXFW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e2a31de3d11410da00ac33bb3ee1ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b87bedce9ba94e93bce039af282df9bc",
              "IPY_MODEL_c935db9e22e54d68a8df4104b2656447",
              "IPY_MODEL_c99f3ef367c24ce3b12208ac9c1f3c40"
            ],
            "layout": "IPY_MODEL_774d725d3fdd45aaa54bde2346397b79"
          }
        },
        "b87bedce9ba94e93bce039af282df9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd4f253551d347baa8b9300a35e77b66",
            "placeholder": "​",
            "style": "IPY_MODEL_12d7b54464274ed293044f2114c3d157",
            "value": "modules.json: 100%"
          }
        },
        "c935db9e22e54d68a8df4104b2656447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a638d79d6fab478bba537044b834edce",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc026e2c50ae4c2f8790355ca34bf18c",
            "value": 349
          }
        },
        "c99f3ef367c24ce3b12208ac9c1f3c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eedf9690338e473887f612650e37262c",
            "placeholder": "​",
            "style": "IPY_MODEL_8ab070b841e442769ef3d08423e80455",
            "value": " 349/349 [00:00&lt;00:00, 31.3kB/s]"
          }
        },
        "774d725d3fdd45aaa54bde2346397b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4f253551d347baa8b9300a35e77b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12d7b54464274ed293044f2114c3d157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a638d79d6fab478bba537044b834edce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc026e2c50ae4c2f8790355ca34bf18c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eedf9690338e473887f612650e37262c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab070b841e442769ef3d08423e80455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ff2543b74c84cbeaba2398b1df76e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e62612a83c8949af9d24c7a13daa8cd9",
              "IPY_MODEL_5ef9b8ba6f2f41a2b11e1d2677eab6f9",
              "IPY_MODEL_05de3532d7144698b285eb14ab0e8a77"
            ],
            "layout": "IPY_MODEL_a823ce0f63c548e89764e02c3abcc7cc"
          }
        },
        "e62612a83c8949af9d24c7a13daa8cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285f50cdb6ba4f49b99670913b10a308",
            "placeholder": "​",
            "style": "IPY_MODEL_3a0ed2edac1c4286a5c204b53545a864",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "5ef9b8ba6f2f41a2b11e1d2677eab6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077dbd7d95ae4b8492ce487b9013d67f",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb682bdfad8b429b844747e038d9d9c9",
            "value": 116
          }
        },
        "05de3532d7144698b285eb14ab0e8a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b53f64dd057499780ca15100b3053e1",
            "placeholder": "​",
            "style": "IPY_MODEL_28fac770ff784418a0c6f6d6ab669539",
            "value": " 116/116 [00:00&lt;00:00, 12.5kB/s]"
          }
        },
        "a823ce0f63c548e89764e02c3abcc7cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "285f50cdb6ba4f49b99670913b10a308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0ed2edac1c4286a5c204b53545a864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "077dbd7d95ae4b8492ce487b9013d67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb682bdfad8b429b844747e038d9d9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b53f64dd057499780ca15100b3053e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28fac770ff784418a0c6f6d6ab669539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ff4522a8f4e4c6685d2acce1e8d1d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_986e27aa0273437e8aee7a155d913e1a",
              "IPY_MODEL_51a3ed0e1d344333addb9221b19d1017",
              "IPY_MODEL_9572f55220a84d10bb6d75793fe7e650"
            ],
            "layout": "IPY_MODEL_0b489e192d724dd485ef9a0f8c32cdee"
          }
        },
        "986e27aa0273437e8aee7a155d913e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_776c893ed5894991a4b5b95bead7acc6",
            "placeholder": "​",
            "style": "IPY_MODEL_f7dfe19510bb45c789bcdff7a135c8ec",
            "value": "README.md: 100%"
          }
        },
        "51a3ed0e1d344333addb9221b19d1017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf01296bb78c4daab38b09537ef3ccac",
            "max": 10415,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5956fbaf32a74005b7403096bb885b2d",
            "value": 10415
          }
        },
        "9572f55220a84d10bb6d75793fe7e650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_104617b4158f43098724f80ced822544",
            "placeholder": "​",
            "style": "IPY_MODEL_99553ab4bf37462b853e02473b60cc2a",
            "value": " 10.4k/10.4k [00:00&lt;00:00, 1.03MB/s]"
          }
        },
        "0b489e192d724dd485ef9a0f8c32cdee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776c893ed5894991a4b5b95bead7acc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dfe19510bb45c789bcdff7a135c8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf01296bb78c4daab38b09537ef3ccac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5956fbaf32a74005b7403096bb885b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "104617b4158f43098724f80ced822544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99553ab4bf37462b853e02473b60cc2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6ae943bb1584b7ea4969fb355bb5007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b657e042fe4e4100a5cfcc1c8d49dec4",
              "IPY_MODEL_2bfe11c9796e4aa296312ee01f04b184",
              "IPY_MODEL_b95e186691b84a5f9cc689723b6b7ca4"
            ],
            "layout": "IPY_MODEL_c3e73d8dcd34435180a66c6d51b21a80"
          }
        },
        "b657e042fe4e4100a5cfcc1c8d49dec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b3f04406c614204944dfcb75291e6a1",
            "placeholder": "​",
            "style": "IPY_MODEL_206c53b4573541e2b53202003a3e64c5",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "2bfe11c9796e4aa296312ee01f04b184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecfdac0da97247a4bbf5a0d8e0adb0e5",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c03e96b392f34f4082a765fd5ba6e57c",
            "value": 53
          }
        },
        "b95e186691b84a5f9cc689723b6b7ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d7fb07d22474c1ea97b903d30c71cd6",
            "placeholder": "​",
            "style": "IPY_MODEL_63aff56de9144b718a3cc9e4de8cee57",
            "value": " 53.0/53.0 [00:00&lt;00:00, 6.12kB/s]"
          }
        },
        "c3e73d8dcd34435180a66c6d51b21a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b3f04406c614204944dfcb75291e6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206c53b4573541e2b53202003a3e64c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecfdac0da97247a4bbf5a0d8e0adb0e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c03e96b392f34f4082a765fd5ba6e57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d7fb07d22474c1ea97b903d30c71cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63aff56de9144b718a3cc9e4de8cee57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92892aa30d524c2680dfff6b37baaa7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b0f336ac97b4550b2faf8ff7b09977a",
              "IPY_MODEL_cbde30158e934cb8b3a074838a20181f",
              "IPY_MODEL_0b731518f3d04f6facaf32d187efa7ae"
            ],
            "layout": "IPY_MODEL_3a2d7174300e41baa4f7d4f44f89f416"
          }
        },
        "0b0f336ac97b4550b2faf8ff7b09977a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b122a5736ac4478841b8e60bd5d9545",
            "placeholder": "​",
            "style": "IPY_MODEL_81a2d43ba7614b15ad97c8e9c2ee9256",
            "value": "config.json: 100%"
          }
        },
        "cbde30158e934cb8b3a074838a20181f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ca2308eea73478ea5da4febdf48b0b1",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa70c2c9877842cd83a19915a429e3c5",
            "value": 571
          }
        },
        "0b731518f3d04f6facaf32d187efa7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d08b5ecbb25948c1b23ebc41889e465e",
            "placeholder": "​",
            "style": "IPY_MODEL_4b5fe49390fe46eea9f5378e18c19d3b",
            "value": " 571/571 [00:00&lt;00:00, 39.9kB/s]"
          }
        },
        "3a2d7174300e41baa4f7d4f44f89f416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b122a5736ac4478841b8e60bd5d9545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a2d43ba7614b15ad97c8e9c2ee9256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ca2308eea73478ea5da4febdf48b0b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa70c2c9877842cd83a19915a429e3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d08b5ecbb25948c1b23ebc41889e465e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5fe49390fe46eea9f5378e18c19d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b252d5d62cd4180a8affc55c7bfea12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b74ac0ca440144928c9eadc3ff2726eb",
              "IPY_MODEL_47c89566918f47f6b35b265e3c68239c",
              "IPY_MODEL_5ee13579ee5e47309e175f5ee029bd9d"
            ],
            "layout": "IPY_MODEL_dcd0083965f14fcf844edb19ccaf72a1"
          }
        },
        "b74ac0ca440144928c9eadc3ff2726eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_958545b668114485b358278903d4e7b2",
            "placeholder": "​",
            "style": "IPY_MODEL_eaafc1bd21734eda8a5835e874efee61",
            "value": "model.safetensors: 100%"
          }
        },
        "47c89566918f47f6b35b265e3c68239c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb681caac7ac4bbf9a62a70721c78a5a",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c995389fa6e24799a3248e60f6dc18bb",
            "value": 437971872
          }
        },
        "5ee13579ee5e47309e175f5ee029bd9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83b693bb785432bb833f68f62b3eebe",
            "placeholder": "​",
            "style": "IPY_MODEL_ec861a7fb0f445ea8d383482b7a99474",
            "value": " 438M/438M [00:02&lt;00:00, 231MB/s]"
          }
        },
        "dcd0083965f14fcf844edb19ccaf72a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958545b668114485b358278903d4e7b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaafc1bd21734eda8a5835e874efee61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb681caac7ac4bbf9a62a70721c78a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c995389fa6e24799a3248e60f6dc18bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e83b693bb785432bb833f68f62b3eebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec861a7fb0f445ea8d383482b7a99474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3b2fe1c8154464e995482729b36b1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_439b2789bc3445c197579ef2edec73d6",
              "IPY_MODEL_610e32588bf94e4d8a0f90bd4f10aec6",
              "IPY_MODEL_b25aa6024e0345f5b6a3e14aa7f43345"
            ],
            "layout": "IPY_MODEL_acf97274b061410285bcdba70e9bf479"
          }
        },
        "439b2789bc3445c197579ef2edec73d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9f2828fc1f04af7b6ae29ba8f828155",
            "placeholder": "​",
            "style": "IPY_MODEL_ffe26cd1df6e4c049721371ccdbe242c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "610e32588bf94e4d8a0f90bd4f10aec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aba79037b0b4dd2a54f026122317248",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c038403342394a39929fbea06b60c0d4",
            "value": 363
          }
        },
        "b25aa6024e0345f5b6a3e14aa7f43345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bf5455904641228bc4864079d9bdbd",
            "placeholder": "​",
            "style": "IPY_MODEL_378d076b947f4b0b8c8d926ff0edf055",
            "value": " 363/363 [00:00&lt;00:00, 24.4kB/s]"
          }
        },
        "acf97274b061410285bcdba70e9bf479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f2828fc1f04af7b6ae29ba8f828155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe26cd1df6e4c049721371ccdbe242c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aba79037b0b4dd2a54f026122317248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c038403342394a39929fbea06b60c0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91bf5455904641228bc4864079d9bdbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378d076b947f4b0b8c8d926ff0edf055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e78c4d1645404ce3928be5618f9080d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_490772156320497fbd030adced8e80ea",
              "IPY_MODEL_31c5e9395a34437595d6cd38ba91b0c5",
              "IPY_MODEL_3242e89ff51a4c4ca5c9a0cf07e58c28"
            ],
            "layout": "IPY_MODEL_3bd1789dc5ec44308181746558056a21"
          }
        },
        "490772156320497fbd030adced8e80ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7034252ce0224d079997dee212e29592",
            "placeholder": "​",
            "style": "IPY_MODEL_a5ab2180d2884ba0b21c001cea005be1",
            "value": "vocab.txt: 100%"
          }
        },
        "31c5e9395a34437595d6cd38ba91b0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d98cb7658930477da534c94f418d7d97",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7973633080b04cd9acc7e5d1a55ffb4d",
            "value": 231536
          }
        },
        "3242e89ff51a4c4ca5c9a0cf07e58c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8531205c56704493a6475dc5a1173d9c",
            "placeholder": "​",
            "style": "IPY_MODEL_0e3efe1e12ca425fa13441e274828dcf",
            "value": " 232k/232k [00:00&lt;00:00, 1.43MB/s]"
          }
        },
        "3bd1789dc5ec44308181746558056a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7034252ce0224d079997dee212e29592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5ab2180d2884ba0b21c001cea005be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d98cb7658930477da534c94f418d7d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7973633080b04cd9acc7e5d1a55ffb4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8531205c56704493a6475dc5a1173d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3efe1e12ca425fa13441e274828dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0315fbc207c04c9c86cf8d0bac521acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e8c3209b83a4d45a424c3a222a500e3",
              "IPY_MODEL_20fd5190b7a049b88910117b1a87e217",
              "IPY_MODEL_1e4ac969371c497ea41705152b351b74"
            ],
            "layout": "IPY_MODEL_26151f43c1a745b9a714f13147223465"
          }
        },
        "9e8c3209b83a4d45a424c3a222a500e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38ff865a1f16418cb352edac8fb47c4d",
            "placeholder": "​",
            "style": "IPY_MODEL_b6664b81e5974bdbb28747b67d4ed0e5",
            "value": "tokenizer.json: 100%"
          }
        },
        "20fd5190b7a049b88910117b1a87e217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dec49595e35a45ad93835e7e5b0f6718",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf1a9809837646aeb1784f7941cf6fb2",
            "value": 466021
          }
        },
        "1e4ac969371c497ea41705152b351b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e33c8d6f28b843a4a34bdf529cb469a6",
            "placeholder": "​",
            "style": "IPY_MODEL_d1f7c12653b14b8b87a5a442480fb50f",
            "value": " 466k/466k [00:00&lt;00:00, 2.82MB/s]"
          }
        },
        "26151f43c1a745b9a714f13147223465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ff865a1f16418cb352edac8fb47c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6664b81e5974bdbb28747b67d4ed0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dec49595e35a45ad93835e7e5b0f6718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf1a9809837646aeb1784f7941cf6fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e33c8d6f28b843a4a34bdf529cb469a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f7c12653b14b8b87a5a442480fb50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6997a665cb947d0bcf4735c30caeeff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90ab017e6c2b42ef8ad362b934a3b601",
              "IPY_MODEL_a6275a61925e49848daf00362f50905a",
              "IPY_MODEL_33321808034147cdbe04a3e704437678"
            ],
            "layout": "IPY_MODEL_cb23c166b12a41f2a42b092a1aafbaaf"
          }
        },
        "90ab017e6c2b42ef8ad362b934a3b601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d5c7ca34894278aa042f46c8799a0c",
            "placeholder": "​",
            "style": "IPY_MODEL_c3ca430974b945c0a2e95233f1329cd3",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a6275a61925e49848daf00362f50905a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b73f5528dd4f5b9b95361eb648d5df",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_498a78fa46654d60a7b10c8219b6578c",
            "value": 239
          }
        },
        "33321808034147cdbe04a3e704437678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ad29dd8a05e4346b40f74028e7e671b",
            "placeholder": "​",
            "style": "IPY_MODEL_37048abb4d834059a30ddb471bbe103d",
            "value": " 239/239 [00:00&lt;00:00, 23.3kB/s]"
          }
        },
        "cb23c166b12a41f2a42b092a1aafbaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d5c7ca34894278aa042f46c8799a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ca430974b945c0a2e95233f1329cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05b73f5528dd4f5b9b95361eb648d5df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498a78fa46654d60a7b10c8219b6578c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ad29dd8a05e4346b40f74028e7e671b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37048abb4d834059a30ddb471bbe103d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "263e52b3eba94730acfacdc88131fee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a75039d2417d4ec69847cdb5c62bcd9b",
              "IPY_MODEL_2fa1c66585c14f48b4577d7599660ef8",
              "IPY_MODEL_c9c04dd514f9489a9e980a92cb4226c5"
            ],
            "layout": "IPY_MODEL_796c7113e8674a4aae9bb63c2e2fb5a6"
          }
        },
        "a75039d2417d4ec69847cdb5c62bcd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_231320141068464288d0d1ed0ed92328",
            "placeholder": "​",
            "style": "IPY_MODEL_8cdd61365453460999d9e39853934e46",
            "value": "config.json: 100%"
          }
        },
        "2fa1c66585c14f48b4577d7599660ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5b265ec8954ac9a8ba930660128b78",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bd9913332814719abf6d8dd7e875a25",
            "value": 190
          }
        },
        "c9c04dd514f9489a9e980a92cb4226c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d998f27d28714d8c855776e05f2637f2",
            "placeholder": "​",
            "style": "IPY_MODEL_723238e3e7f14df99139b1c0f50f2424",
            "value": " 190/190 [00:00&lt;00:00, 20.0kB/s]"
          }
        },
        "796c7113e8674a4aae9bb63c2e2fb5a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "231320141068464288d0d1ed0ed92328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cdd61365453460999d9e39853934e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b5b265ec8954ac9a8ba930660128b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd9913332814719abf6d8dd7e875a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d998f27d28714d8c855776e05f2637f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723238e3e7f14df99139b1c0f50f2424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhoomiAgrawal12/BhoomiAgrawal12/blob/main/mistralAIRag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r-wBwlfvR0j",
        "outputId": "5e2af224-8d44-4db0-8914-ebb784824e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate\n",
            "  Downloading weaviate-0.1.2-py3-none-any.whl.metadata (296 bytes)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting rapidocr-onnxruntime\n",
            "  Downloading rapidocr_onnxruntime-1.4.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr-onnxruntime)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (4.11.0.86)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (2.0.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (11.1.0)\n",
            "Collecting onnxruntime>=1.7.0 (from rapidocr-onnxruntime)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime) (4.67.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading weaviate-0.1.2-py3-none-any.whl (2.6 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr_onnxruntime-1.4.4-py3-none-any.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: weaviate, pyclipper, pypdf, humanfriendly, tiktoken, coloredlogs, onnxruntime, rapidocr-onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.0 pyclipper-1.3.0.post6 pypdf-5.3.1 rapidocr-onnxruntime-1.4.4 tiktoken-0.9.0 weaviate-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install weaviate langchain tiktoken pypdf rapidocr-onnxruntime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "WEAVIATE_API_KEY=userdata.get('WEAVIATE_API_KEY')\n",
        "WEAVIATE_CLUSTER_URL=userdata.get('WEAVIATE_CLUSTER_URL')"
      ],
      "metadata": {
        "id": "kUBzIMN8vlGe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community weaviate-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl0myVKD1Edg",
        "outputId": "282afdb7-14e1-478f-c6cf-cda1e720ac4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting weaviate-client\n",
            "  Downloading weaviate_client-4.11.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.43)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.13)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (0.28.1)\n",
            "Collecting validators==0.34.0 (from weaviate-client)\n",
            "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting authlib<1.3.2,>=1.2.1 (from weaviate-client)\n",
            "  Downloading Authlib-1.3.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (2.10.6)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.71.0)\n",
            "Collecting grpcio-tools<2.0.0,>=1.66.2 (from weaviate-client)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.66.2 (from weaviate-client)\n",
            "  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client) (43.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (2.22)\n",
            "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weaviate_client-4.11.1-py3-none-any.whl (353 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.3/353.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Authlib-1.3.1-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.8/223.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading grpcio_health_checking-1.71.0-py3-none-any.whl (18 kB)\n",
            "Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: validators, python-dotenv, protobuf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, grpcio-tools, grpcio-health-checking, pydantic-settings, dataclasses-json, authlib, weaviate-client, langchain-community\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "Successfully installed authlib-1.3.1 dataclasses-json-0.6.7 grpcio-health-checking-1.71.0 grpcio-tools-1.71.0 httpx-sse-0.4.0 langchain-community-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 protobuf-5.29.3 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0 validators-0.34.0 weaviate-client-4.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client==3.23.2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W02kkega2nK9",
        "outputId": "42529604-6624-4c0e-b560-faaeea4ea2f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate-client==3.23.2\n",
            "  Downloading weaviate_client-3.23.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting requests<=2.31.0,>=2.28.0 (from weaviate-client==3.23.2)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting validators<=0.21.0,>=0.18.2 (from weaviate-client==3.23.2)\n",
            "  Downloading validators-0.21.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.59.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client==3.23.2) (4.67.1)\n",
            "Requirement already satisfied: authlib>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client==3.23.2) (1.3.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib>=1.1.0->weaviate-client==3.23.2) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<=2.31.0,>=2.28.0->weaviate-client==3.23.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<=2.31.0,>=2.28.0->weaviate-client==3.23.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<=2.31.0,>=2.28.0->weaviate-client==3.23.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<=2.31.0,>=2.28.0->weaviate-client==3.23.2) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib>=1.1.0->weaviate-client==3.23.2) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib>=1.1.0->weaviate-client==3.23.2) (2.22)\n",
            "Downloading weaviate_client-3.23.2-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.21.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: validators, requests, weaviate-client\n",
            "  Attempting uninstall: validators\n",
            "    Found existing installation: validators 0.34.0\n",
            "    Uninstalling validators-0.34.0:\n",
            "      Successfully uninstalled validators-0.34.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: weaviate-client\n",
            "    Found existing installation: weaviate-client 4.11.1\n",
            "    Uninstalling weaviate-client-4.11.1:\n",
            "      Successfully uninstalled weaviate-client-4.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.31.0 validators-0.21.0 weaviate-client-3.23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR LATEST VERSION OF WEAVIATE\n",
        "# from langchain_community.vectorstores import Weaviate\n",
        "# import weaviate\n",
        "\n",
        "# from weaviate import WeaviateClient,ConnectionParams,ProtocolParams\n",
        "# from weaviate.auth import AuthApiKey\n",
        "# WEAVIATE_CLUSTER_URL = userdata.get(\"WEAVIATE_CLUSTER_URL\")\n",
        "# WEAVIATE_API_KEY = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "# connection_params = ConnectionParams(\n",
        "#     http=ProtocolParams(\n",
        "#         host=WEAVIATE_CLUSTER_URL,\n",
        "#         port=443,  # Use 443 for secure HTTPS\n",
        "#         secure=True\n",
        "#     ),\n",
        "#     grpc=ProtocolParams(\n",
        "#         host=WEAVIATE_CLUSTER_URL,\n",
        "#         port=50051,  # Standard gRPC port\n",
        "#         secure=True\n",
        "#     ),\n",
        "# )\n",
        "# # Initialize client with proper authentication\n",
        "# client = WeaviateClient(\n",
        "#     connection_params=connection_params,\n",
        "#     auth_client_secret=AuthApiKey(WEAVIATE_API_KEY),\n",
        "# )"
      ],
      "metadata": {
        "id": "jygsaWIYxLbR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "20f9d43e-d2ab-4906-8cb8-4e7752ab0774"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-800ee84bdd0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeaviate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweaviate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mweaviate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeaviateClient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mConnectionParams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mProtocolParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mweaviate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAuthApiKey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Weaviate\n",
        "import weaviate\n",
        "from weaviate.auth import AuthApiKey\n",
        "\n",
        "# Fetch credentials\n",
        "WEAVIATE_CLUSTER_URL = userdata.get(\"WEAVIATE_CLUSTER_URL\")\n",
        "WEAVIATE_API_KEY = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "\n",
        "# Initialize Weaviate client for 3.x versions\n",
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_CLUSTER_URL,  # Use the direct URL\n",
        "    auth_client_secret=AuthApiKey(WEAVIATE_API_KEY),\n",
        "    additional_headers={\"X-OpenAI-Api-Key\": WEAVIATE_API_KEY},  # If using OpenAI integration\n",
        ")\n"
      ],
      "metadata": {
        "id": "wwoYk04rFlEF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VC-qUUG6x18S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to fix unicode error in google colab\n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "xKVMVvQqxo49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrM41Lwx5feb",
        "outputId": "7fe79020-0584-471b-ab15-98e656598f76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m768.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings=HuggingFaceEmbeddings(model_name=embedding_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "6e2a31de3d11410da00ac33bb3ee1ca8",
            "b87bedce9ba94e93bce039af282df9bc",
            "c935db9e22e54d68a8df4104b2656447",
            "c99f3ef367c24ce3b12208ac9c1f3c40",
            "774d725d3fdd45aaa54bde2346397b79",
            "cd4f253551d347baa8b9300a35e77b66",
            "12d7b54464274ed293044f2114c3d157",
            "a638d79d6fab478bba537044b834edce",
            "bc026e2c50ae4c2f8790355ca34bf18c",
            "eedf9690338e473887f612650e37262c",
            "8ab070b841e442769ef3d08423e80455",
            "0ff2543b74c84cbeaba2398b1df76e91",
            "e62612a83c8949af9d24c7a13daa8cd9",
            "5ef9b8ba6f2f41a2b11e1d2677eab6f9",
            "05de3532d7144698b285eb14ab0e8a77",
            "a823ce0f63c548e89764e02c3abcc7cc",
            "285f50cdb6ba4f49b99670913b10a308",
            "3a0ed2edac1c4286a5c204b53545a864",
            "077dbd7d95ae4b8492ce487b9013d67f",
            "eb682bdfad8b429b844747e038d9d9c9",
            "5b53f64dd057499780ca15100b3053e1",
            "28fac770ff784418a0c6f6d6ab669539",
            "6ff4522a8f4e4c6685d2acce1e8d1d4e",
            "986e27aa0273437e8aee7a155d913e1a",
            "51a3ed0e1d344333addb9221b19d1017",
            "9572f55220a84d10bb6d75793fe7e650",
            "0b489e192d724dd485ef9a0f8c32cdee",
            "776c893ed5894991a4b5b95bead7acc6",
            "f7dfe19510bb45c789bcdff7a135c8ec",
            "cf01296bb78c4daab38b09537ef3ccac",
            "5956fbaf32a74005b7403096bb885b2d",
            "104617b4158f43098724f80ced822544",
            "99553ab4bf37462b853e02473b60cc2a",
            "b6ae943bb1584b7ea4969fb355bb5007",
            "b657e042fe4e4100a5cfcc1c8d49dec4",
            "2bfe11c9796e4aa296312ee01f04b184",
            "b95e186691b84a5f9cc689723b6b7ca4",
            "c3e73d8dcd34435180a66c6d51b21a80",
            "5b3f04406c614204944dfcb75291e6a1",
            "206c53b4573541e2b53202003a3e64c5",
            "ecfdac0da97247a4bbf5a0d8e0adb0e5",
            "c03e96b392f34f4082a765fd5ba6e57c",
            "9d7fb07d22474c1ea97b903d30c71cd6",
            "63aff56de9144b718a3cc9e4de8cee57",
            "92892aa30d524c2680dfff6b37baaa7f",
            "0b0f336ac97b4550b2faf8ff7b09977a",
            "cbde30158e934cb8b3a074838a20181f",
            "0b731518f3d04f6facaf32d187efa7ae",
            "3a2d7174300e41baa4f7d4f44f89f416",
            "7b122a5736ac4478841b8e60bd5d9545",
            "81a2d43ba7614b15ad97c8e9c2ee9256",
            "2ca2308eea73478ea5da4febdf48b0b1",
            "fa70c2c9877842cd83a19915a429e3c5",
            "d08b5ecbb25948c1b23ebc41889e465e",
            "4b5fe49390fe46eea9f5378e18c19d3b",
            "0b252d5d62cd4180a8affc55c7bfea12",
            "b74ac0ca440144928c9eadc3ff2726eb",
            "47c89566918f47f6b35b265e3c68239c",
            "5ee13579ee5e47309e175f5ee029bd9d",
            "dcd0083965f14fcf844edb19ccaf72a1",
            "958545b668114485b358278903d4e7b2",
            "eaafc1bd21734eda8a5835e874efee61",
            "bb681caac7ac4bbf9a62a70721c78a5a",
            "c995389fa6e24799a3248e60f6dc18bb",
            "e83b693bb785432bb833f68f62b3eebe",
            "ec861a7fb0f445ea8d383482b7a99474",
            "e3b2fe1c8154464e995482729b36b1c6",
            "439b2789bc3445c197579ef2edec73d6",
            "610e32588bf94e4d8a0f90bd4f10aec6",
            "b25aa6024e0345f5b6a3e14aa7f43345",
            "acf97274b061410285bcdba70e9bf479",
            "c9f2828fc1f04af7b6ae29ba8f828155",
            "ffe26cd1df6e4c049721371ccdbe242c",
            "8aba79037b0b4dd2a54f026122317248",
            "c038403342394a39929fbea06b60c0d4",
            "91bf5455904641228bc4864079d9bdbd",
            "378d076b947f4b0b8c8d926ff0edf055",
            "e78c4d1645404ce3928be5618f9080d1",
            "490772156320497fbd030adced8e80ea",
            "31c5e9395a34437595d6cd38ba91b0c5",
            "3242e89ff51a4c4ca5c9a0cf07e58c28",
            "3bd1789dc5ec44308181746558056a21",
            "7034252ce0224d079997dee212e29592",
            "a5ab2180d2884ba0b21c001cea005be1",
            "d98cb7658930477da534c94f418d7d97",
            "7973633080b04cd9acc7e5d1a55ffb4d",
            "8531205c56704493a6475dc5a1173d9c",
            "0e3efe1e12ca425fa13441e274828dcf",
            "0315fbc207c04c9c86cf8d0bac521acd",
            "9e8c3209b83a4d45a424c3a222a500e3",
            "20fd5190b7a049b88910117b1a87e217",
            "1e4ac969371c497ea41705152b351b74",
            "26151f43c1a745b9a714f13147223465",
            "38ff865a1f16418cb352edac8fb47c4d",
            "b6664b81e5974bdbb28747b67d4ed0e5",
            "dec49595e35a45ad93835e7e5b0f6718",
            "bf1a9809837646aeb1784f7941cf6fb2",
            "e33c8d6f28b843a4a34bdf529cb469a6",
            "d1f7c12653b14b8b87a5a442480fb50f",
            "d6997a665cb947d0bcf4735c30caeeff",
            "90ab017e6c2b42ef8ad362b934a3b601",
            "a6275a61925e49848daf00362f50905a",
            "33321808034147cdbe04a3e704437678",
            "cb23c166b12a41f2a42b092a1aafbaaf",
            "a1d5c7ca34894278aa042f46c8799a0c",
            "c3ca430974b945c0a2e95233f1329cd3",
            "05b73f5528dd4f5b9b95361eb648d5df",
            "498a78fa46654d60a7b10c8219b6578c",
            "1ad29dd8a05e4346b40f74028e7e671b",
            "37048abb4d834059a30ddb471bbe103d",
            "263e52b3eba94730acfacdc88131fee6",
            "a75039d2417d4ec69847cdb5c62bcd9b",
            "2fa1c66585c14f48b4577d7599660ef8",
            "c9c04dd514f9489a9e980a92cb4226c5",
            "796c7113e8674a4aae9bb63c2e2fb5a6",
            "231320141068464288d0d1ed0ed92328",
            "8cdd61365453460999d9e39853934e46",
            "4b5b265ec8954ac9a8ba930660128b78",
            "2bd9913332814719abf6d8dd7e875a25",
            "d998f27d28714d8c855776e05f2637f2",
            "723238e3e7f14df99139b1c0f50f2424"
          ]
        },
        "id": "yX14OyN5xv5q",
        "outputId": "c5b4a181-8b18-4faf-861f-b0d820f32102"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-43e6c256e608>:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings=HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e2a31de3d11410da00ac33bb3ee1ca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ff2543b74c84cbeaba2398b1df76e91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ff4522a8f4e4c6685d2acce1e8d1d4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6ae943bb1584b7ea4969fb355bb5007"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92892aa30d524c2680dfff6b37baaa7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b252d5d62cd4180a8affc55c7bfea12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3b2fe1c8154464e995482729b36b1c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e78c4d1645404ce3928be5618f9080d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0315fbc207c04c9c86cf8d0bac521acd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6997a665cb947d0bcf4735c30caeeff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "263e52b3eba94730acfacdc88131fee6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buKkb0-2ZoTh",
        "outputId": "9d5cd0d1-0a44-45a1-a146-4f86b493839a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader=PyPDFLoader(\"/content/2404.18144v1.pdf\",extract_images=True)\n",
        "pages=loader.load()"
      ],
      "metadata": {
        "id": "sOwZZfney2Pf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLtKCENc6vBu",
        "outputId": "80552457-4f0c-4b08-b1fa-7edb142839b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 0, 'page_label': '1'}, page_content='Generative AI for Visualization: State of the Art and\\nFuture Directions\\nYilin Yea,b, Jianing Haoa, Yihan Houa, Zhan Wanga, Shishi Xiaoa, Yuyu\\nLuoa,b, Wei Zenga,b\\naThe Hong Kong University of Science and Technology\\n(Guangzhou), Guangzhou, Guangdong, China\\nbThe Hong Kong University of Science and Technology, Hong Kong SAR, China\\nAbstract\\nGenerative AI (GenAI) has witnessed remarkable progress in recent years\\nand demonstrated impressive performance in various generation tasks in dif-\\nferent domains such as computer vision and computational design. Many\\nresearchers have attempted to integrate GenAI into visualization framework,\\nleveraging the superior generative capacity for different operations. Concur-\\nrently, recent major breakthroughs in GenAI like diffusion model and large\\nlanguage model have also drastically increase the potential of GenAI4VIS.\\nFrom a technical perspective, this paper looks back on previous visualization\\nstudies leveraging GenAI and discusses the challenges and opportunities for\\nfuture research. Specifically, we cover the applications of different types of\\nGenAI methods including sequence, tabular, spatial and graph generation\\ntechniques for different tasks of visualization which we summarize into four\\nmajor stages: data enhancement, visual mapping generation, stylization and\\ninteraction. For each specific visualization sub-task, we illustrate the typi-\\ncal data and concrete GenAI algorithms, aiming to provide in-depth under-\\nstanding of the state-of-the-art GenAI4VIS techniques and their limitations.\\nFurthermore, based on the survey, we discuss three major aspects of chal-\\nlenges and research opportunities including evaluation, dataset, and the gap\\nbetween end-to-end GenAI and generative algorithms. By summarizing dif-\\nferent generation algorithms, their current applications and limitations, this\\npaper endeavors to provide useful insights for future GenAI4VIS research.\\nKeywords: Visualization, Generative AI\\nPreprint submitted to Visual Informatics April 30, 2024\\narXiv:2404.18144v1  [cs.LG]  28 Apr 2024'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 1, 'page_label': '2'}, page_content='1. Introduction\\nVizDeck [1]. Visualization is a process of rendering graphical representa-\\ntions of spatial or abstract data to assist exploratory data analysis. Recently,\\nmany researchers have attempted to apply artificial intelligence (AI) for visu-\\nalization tasks [2, 3, 4, 5, 6]. Particularly, as visualization essentially involves\\nrepresentations and interactions for raw data, many visualization researchers\\nhave started to adopt the rapidly developing generative AI (GenAI) tech-\\nnology, a type of AI technology that empowers the generation of synthetic\\ncontent and data by learning from existing man-made samples [7, 8]. GenAI\\nhas come to the foreground of artificial intelligence in recent years, with pro-\\nfound and widespread impact on various research and application domains\\nsuch as artifact and interaction design ( e.g. [9, 10, 11]).\\nRecently, multi-modal AI generation model such as Stable Diffusion [12]\\nor DaLL-E 2 [13] enable laymen users without traditional art and design\\nskills to easily produce high-quality digital paintings or designs with simple\\ntext prompts. In natural language generation, large language models like\\nGPT [14] and LLaMa [15] also demonstrate astounding power of conver-\\nsation, reasoning and knowledge embedding. In computer graphics, recent\\nmodels like DreamFusion [16] also shows impressive potential in 3D genera-\\ntion. GenAI’s unique strength lies in its flexible capacity to model data and\\ngenerate designs based on implicitly embedded knowledge gleaned from real-\\nworld data. This characteristic positions GenAI as a transformative force\\ncapable of alleviating the workload and complexity associated with tradi-\\ntional computational methods, and extending the diversity of design with\\nmore creative generated results than previous methods.\\nThe burgeoning potential of GenAI is particularly evident in its ability\\nto enhance and streamline operations throughout the data visualization pro-\\ncess. From data processing to the mapping stage and beyond, GenAI can\\nplay a pivotal role in tasks such as data inference and augmentation, auto-\\nmatic visualization generation, and chart question answering. For instance,\\nthe automatic visualization generation has been a longstanding research fo-\\ncus predating the current wave of GenAI methods, offering non-expert users\\nan efficient means of conducting data analysis and crafting visual representa-\\ntions (e.g., [17, 18]). Traditionally, automatic visualization approaches relied\\non expert-designed rules rooted in design principles [19]. However, these\\nmethods were shackled by the constraints of knowledge-based systems [20],\\nstruggling to comprehensively incorporate expert knowledge within convo-\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 2, 'page_label': '3'}, page_content='luted rules or oversimplified objective functions. The advent of GenAI in-\\ntroduces a paradigm shift, promising not only increased efficiency but also a\\nmore intuitive and accessible approach to visualization in an era marked by\\nunprecedented technological advancements.\\nDespite the impressive capability of GenAI, when applied to visualiza-\\ntion it can face many challenges because of its unique data structure and\\nanalytic requirements. For example, the generation of visualization images\\nis significantly different from generation of natural or artistic images. First,\\nthe evaluation of GenAI for visualization tasks is more complex than nat-\\nural image generation as many factors beyond image similarity need to be\\nconsidered, such as efficiency [21] and data integrity [22]. Second, compared\\nto general GenAI tasks trained on large datasets with simple annotations,\\nthe diversity and complexity of visualization tasks demand more complex\\ntraining data [23], which is harder to curate. Third, the gap between the\\ntraditional visualization pipeline with strong rule-based constraints makes\\nit difficult to fully integrate with end-to-end GenAI methods. These unique\\ncharacteristics makes it less straightforward to leverage the latest pre-trained\\nGenAI models in general domain to empower visualization-specific genera-\\ntion. Therefore, it is important to understand how previous works have uti-\\nlized GenAI for various visualization applications, what challenges are met\\nand especially how the GenAI methods are adapted to the tasks.\\nAlthough some previous surveys have covered the use of AI in a general\\nsense for visualization [3], to the best of our knowledge, no study has focused\\non comprehensive review of GenAI methods used in visualization. This sur-\\nvey extensively reviews the literature and summarizes the AI-powered gener-\\nation methods developed for visualization. We categorize the various GenAI\\nmethods according to the concrete tasks they address, which correspond to\\ndifferent stages of visualization generation. In this way, we manage to col-\\nlect 81 research papers on GenAI4VIS. We particularly focus on the different\\nalgorithms used in specific tasks in the hope of helping researchers under-\\nstand the state-of-the-art technical development as well as challenges. We\\nalso discuss and highlight potential research opportunities.\\nThis paper is structured as follows. Section 2 outlines the scope and\\ntaxonomy of our survey with definition of key concepts. Starting from Sec-\\ntion 3 to Section 6, each section corresponds to a stage in the visualization\\npipeline where GenAI has been used. Specifically, Section 3 concerns the\\nuse of GenAI for data enhancement. Section 4 summarizes works leveraging\\nGenAI for visual mapping generation. Section 5 focuses on how GenAI is uti-\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 3, 'page_label': '4'}, page_content='lized for stylization and communication with visualization. Section 6 covers\\nGenAI techniques to support user interaction. Each subsection in Section 3\\nto Section 6 covers a specific task in the stage. Instead of listing the works\\none by one, the structure of the subsection is divided into two parts: data\\n& algorithm and discussion, for a comprehensive understanding of how the\\ncurrent GenAI method works for data of certain structures and what remains\\nchallenging for GenAI in particular tasks. Finally, Section 7 discusses some\\ndominant challenges and research opportunities for future research.\\n2. Scope and Taxonomy\\n2.1. Scope and Definition\\nGenerative AI (GenAI) is a type of AI technique that generates synthetic\\nartifacts by analyzing training examples; learning their patterns and distribu-\\ntion; and then creating realistic facsimiles. GenAI uses generative modeling\\nand advances in deep learning (DL) to produce diverse content at scale by\\nutilizing existing media such as text, graphics, audio, and video [7, 8]. A\\nkey feature of GenAI is that it generates new content by learning from data\\ninstead of explicit programs.\\nGenAI methods categorization. Despite the differences between differ-\\nent domain targets of generation ranging from text, code, multi-media to 3D\\ngeneration, the particular algorithms of generation actually depend on the\\ndata structures which show common characteristics across different domains.\\nParticularly, in GenAI4VIS applications, categorization based on data struc-\\ntures can facilitate more concrete understanding of the algorithms in relation\\nto the different types of data involved in different visualization tasks. Here,\\nwe provide an overview of different types of GenAI in terms of typical data\\nstructures associated with data visualization.\\n• Sequence Generation: This category includes the generation of or-\\ndered data, such as text, code, music, videos, and time-series data.\\nSequence generation models, like LSTMs and Transformers, can be\\nused to create content with a sequential or temporal structure.\\n• Tabular Generation: This category covers the generation of struc-\\ntured data in the form of rows and columns, such as spreadsheets or\\ndatabase tables. Applications include data augmentation, anonymiza-\\ntion, and data imputation.\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 4, 'page_label': '5'}, page_content='Generative AI\\nRaw Data\\nTransformed Data Visual Structure\\n Views\\nData Enhancement\\nSequence\\nTabularSpatial\\nGraph\\nInteraction\\nVisualization Retrieval \\nChart Question Answering\\nVisual Mapping\\nVisual Primitive Generation\\nData Inference\\nData Embedding\\nStylization\\nStyle Transfer\\nChart Embellishment\\nTextual Annotation\\nTimeline & Story\\nFigure 1: The overview of GenAI4VIS applications for different visualization tasks, in-\\ncluding data enhancement, visual mapping generation, stylization and interaction tasks.\\n• Graph Generation: This category involves generating graph and net-\\nwork structures, such as social networks, molecular structures, or rec-\\nommendation systems. Models like Graph Neural Networks (GNNs)\\nand Graph Convolutional Networks (GCNs) can be used to generate\\nor manipulate graph-structured data.\\n• Spatial Generation : This category encompasses the generation of\\nboth 2D images and 3D models. These data have the common char-\\nacteristics of spatial data in 3D or 2D projection in Euclidean spaces,\\nwhich can be represented as pixels, voxels or points with 2D/3D co-\\nordinates. 2D generation includes image synthesis, style transfer, and\\ndigital art, while 3D generation covers computer graphics, virtual real-\\nity, and 3D printing. Techniques like GANs, VAEs, and PointNet [24]\\ncan be used for creating 2D and 3D content.\\nGenAI4VIS tasks categorization. To categorize and organize the col-\\nlected articles, we are inspired by the classical visualization pipeline describ-\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 5, 'page_label': '6'}, page_content='ing different essential stages [25]. However, as GenAI is utilized in broader\\nscenarios different from traditional operations, we also modify the pipeline\\nto encompass some latest research topics. including data enhancement,\\nvisual mapping generation, stylization, and interaction. Notably, the\\ndata transformation part is generalized to the concept of data enhance-\\nment inspired by the terminology in the study by McNabb et al. [26]. In\\naddition, as few GenAI for visualization works focus on the basic view trans-\\nformation, we replace this part with a broader concept of stylization &\\ncommunication. Under different stages we further categorize the works\\ninto specific tasks, as shown in Figure 1.\\n• Data enhancement. Data enhancement refers to the process of im-\\nproving the quality or completeness of the data or enhancing the feature\\nrepresentation of the data for subsequent visualization. This can in-\\nvolve data augmentation, embedding or other transformations to make\\nit more suitable for visualization.\\n• Visual mapping generation. This refers to the use of algorithms and\\nsoftware tools to generate visualizations automatically without exten-\\nsive manual intervention. Automatic visual mapping generation allows\\nusers to leverage knowledge about how to create appropriate visualiza-\\ntion as common wisdom to reduce the workload and man-made viola-\\ntion of design principles.\\n• Stylization. Extending the concept of presentation in [27], we define\\nstylization in visualization, which involves the application of design\\nprinciples and aesthetic choices to make the visualization more engaging\\nand effective in conveying information. It includes decisions about color\\nschemes, fonts, layout, and other visual or textual elements to enhance\\nthe information-assisted visualization [20].\\n• Interaction. In the context of data visualization, interaction refers\\nto the dynamic engagement and communication between users and the\\nvisualized data. It involves the ability of users to manipulate, explore,\\nand interpret visual representations. This can involve various forms\\nof interactivity, such as graphical interactions like zooming, panning,\\nclicking and natural language interaction like chart question answering.\\nEarlier methods for these tasks focus on rule-based algorithms with complex\\nexpert-designed rules reflecting design principles, which is still effective in\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 6, 'page_label': '7'}, page_content='many applications such as colormap generation [28]. Some studies also lever-\\nage optimization-based methods to minimize expert-defined explicit objective\\nfunctions. However, these types of methods differ from GenAI methods in\\nthat they are top-down and do not learn from real-world data. To narrow\\ndown the scope of our survey, we exclude all previous generative algorithms\\nthat are purely based on rules or optimization.\\nRelation between different GenAI methods and tasks. Due to the\\nwide range of diverse applications in GenAI4VIS, there is no clear-cut one-\\nto-one relation between the type of GenAI methods and the tasks. Neverthe-\\nless, we can observe some interesting correlation. First, sequence generation\\nis mostly applied in visual mapping or interaction-related tasks. This is\\nbecause GenAI such as translation models and the latest LLMs or vision-\\nlanguage model are useful in generating sequence of code specifying visual\\nmapping or sequence of interaction flow and output. Second, tabular gen-\\neration is mostly used in data enhancement. This is because tabular data\\nwith attribute columns are the most common initial input data to visualiza-\\ntion, which benefits from data enhancement like surrogate data generation\\nfor subsequent tasks. Next, graph generation is also mostly used in data\\nenhancement because data inference and augmentation can facilitate subse-\\nquent analysis of graph data. However, despite its relatively rare use, it holds\\ngreat potential for visual mapping and stylization because graphical structure\\nsuch as knowledge graph or scene graph can benefit optimization of visual\\nencodings and layout. Finally, spatial generation is mostly applied in data\\nenhancement and stylization tasks. This is because 2D and 3D data such\\nas images and volumetric data are also common types of input for VIS4AI\\nand SciVis applications, while the embellishment of basic charts into stylized\\ncharts relies on image-based generation methods. Figure 2 illustrates the\\nrelation between GenAI4VIS tasks and methods with a sankey diagram and\\nexemplifies the specific data types that are involved in different methods.\\nTable 1 further list the detailed methodologies for each data structure and\\ntask.\\n2.2. Related Survey\\nSome previous surveys cover the applications of artificial intelligence or\\nmachine learning in general to information visualization or scientific visual-\\nization [2, 3, 58, 4, 5]. Wu et al. [3] surveyed the development of artificial in-\\ntelligence technologies applied to information visualization, focusing on three\\nmajor aspects including visualization data and representation (what), goals\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 7, 'page_label': '8'}, page_content='Data Enhancement\\nTasks\\nVisual Mapping\\nStylization\\nInteraction\\nMethods\\nSpatial\\nTabular\\nGraph\\nSequence\\nSpecific Data Types\\nVisualization code [28],\\nTextual annotation [34],\\nChart question answer [105]\\nRelational data [63], Design choices [38]\\nChemical data [75], Simulation data [39],\\nKnowledge graph [41], SVG [44]\\nVisualization images [21], Sketch [50], \\nQR code images [48], Density map [51], \\nNatural images [47], volumetric data [86],\\nSpatial-temporal data [70], \\nFigure 2: Relation between tasks and methods and examples of specific data types involved\\nin different methods.\\n(why) and specific tasks (how) which concern the use of AI. Other previous\\nsurveys touch upon the use of AI in more specific sub-areas of visualization\\nresearch, such as natural language interface and data story telling [27, 59, 60].\\nShen et al. [27] summarizes all the existing technologies supporting natural\\nlanguage interface to different stages of data visualization, including both tra-\\nditional rule-based techniques and recent AI-powered methods. Bartolomeo\\net al. [61] envisions the potential of GenAI applied to different stages of vi-\\nsualization, mostly focused on interviewing experts and discussing usage sce-\\nnarios. Another closely relevant survey [62] recently focuses on the two-way\\nrelationship between visualization and foundation AI models, which include\\nsome large scale GenAI models like GPT. In comparison, the focus of our\\nsurvey is on the concrete technical advancements and challenges of GenAI\\nmodels for visualization applications.\\nNo previous survey has been dedicated to the various types of GenAI\\nmethods in the context of visualization tasks. Our survey aims to provide a\\nspecialized and comprehensive overview of the GenAI techniques that have\\nbeen used to generate various data or intermediate representations that are\\nuseful for visualization. We also outline challenges and opportunities for\\nfuture research on GenAI for VIS.\\n2.3. Survey Methodology\\nWe combine search-based and reference-driven methods to discover rel-\\nevant literature. We first collect relevant papers from previous surveys and\\nrecent works. Then we expand the list by going through the papers’ ref-\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 8, 'page_label': '9'}, page_content='Table 1: Examples of Specific GenAI4VIS methods applied to different tasks and data\\ntypes.\\nData\\nEnhancement Visual Mapping Stylization Interaction\\nSequence -\\nRNN [29], Deep Q\\nNetwork [30],\\nTransformer [31],\\nLLM [32]\\nLLM [33], RL [34],\\nDetection Net-\\nwork+Template [35]\\nDetection-based\\nModels [36],\\nVision-Language\\nModels [37]\\nTabular Table-GAN [38]\\nFFN [39], Enumer-\\nation+Scoring\\nNetwork [17]\\n- -\\nGraph GNN [40], Latent\\nTraversal [41] KG embedding [42]\\nGraph Latent [43],\\nGraph Style\\nExtraction\\nNetwork [44]\\nGraph Contrastive\\nLearning [45]\\nSpatial\\nVAE [46], GAN [47],\\nDRL [48],\\nBASNet [49],\\nISN [50]\\nFaster R-CNN [51],\\nGAN [52]\\nColor Extraction\\nNetwork [53],\\nSiamese\\nNetwork [54],\\nDiffusion [22],\\nRL [55]\\nTriplet\\nAutoencoder [56],\\nContrastive\\nLearning [57]\\nerences and citations. In this process, we also supplement the results by\\nsearching with key words in the titles of previously collected papers in both\\nthe ACM and IEEE libraries as well as arxiv. In the paper selection process,\\nwe manually filter out the non-GenAI traditional methods such as purely\\nrule-based or optimization-based methods, stressing the key characteristics\\nof GenAI which has learned from real data in self-supervised pre-training\\nor supervised training stages. In total we collect 81 papers as listed in Ta-\\nble 2 utilizing GenAI for visualization tasks. As shown in Table 2, different\\ntypes of GenAI4VIS techniques include sequence generation, tabular genera-\\ntion, spatial generation and graph generation, which can benefit visualization\\ntasks in different stages such as data enhancement, visual mapping, styliza-\\ntion and interaction. Sequence and spatial generation are more often used\\nas they concern the general visualization code, images and natural language\\ninteraction. We acknowledge that our search method may not be exhaustive\\ndue to the manually collection through keyword search and citation traversal.\\nTherefore, this survey mainly provides a comprehensive overview of state-of-\\nthe-art GenAI4VIS methods, where application papers with similar methods\\nmay not be enumerated.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 9, 'page_label': '10'}, page_content='Table 2: Survey taxonomy and example papers. We classify the collected GenAI4VIS\\npapers into different sub-tasks along visualization pipeline and further classify different\\nGenAI methods into sequence, tabular, spatial and graph generation.\\nTasks Subtasks Description Examples\\nData Enhance Data Inference Increase samples or\\ndimensions\\ntabular [63] [64] [38]\\n[65] [66] [67], spatial\\n[46] [68] [48] [69] [70] [71]\\n[47] [72] [73] [69] [48] [74],\\ngraph [75] [41] [76] [40]\\nData Embedding Embed data to hide\\ninformation spatial [50] [49] [77]\\nVisual Mapping Visual Primitive Generate basic visual\\nstructures\\nsequence [29] [30] [78] [31]\\n[79] [80] [32] [81] [82] [83]\\n[84] [85], tabular [39] [17] [86]\\n[18], spatial [51] [52] [87] [88]\\n[89] [90], graph [42]\\nStylization\\nStyle Transfer Imitate styles of\\nexamples\\nspatial [53] [91] [92] [93] [54]\\n[94], graph [43] [95] [44]\\nEmbellishment Generate infographics spatial [32] [22] [96]\\nText Annotation Add information with\\ntext sequence [35, 33, 97]\\nTimeline & Story Generate data story sequence [98] [99] [100] [101]\\n[34], spatial [55], graph [100]\\nInteract Retrieval Find similar charts spatial [57] [56], graph [45]\\nCQA Answer questions\\nabout chart\\nsequence [102] [103], [104] [36]\\n[105] [106] [107] [108] [37]\\n[109]\\n3. Data Enhancement\\n3.1. Data Inference\\nGenAI can be useful for inferring unobserved data items or data features\\nbased on the distributions and feature values of existing data, such as data\\ninterpolation, data augmentation and super-resolution, which we use a gen-\\neral term data inference to describe, as these tasks all aim at inferring unseen\\ndata.\\n3.1.1. Graph Generation\\nData. GenAI for graph data inference is commonly applied in the domain of\\nchemical data.\\n• Chemical data. GenAI-powered data interpolation has been used for\\ninteractive exploration of chemical data [110, 75] to assist discovery of\\nnew molecule structures. For example, ChemoVerse [75] is an inter-\\nactive system that leverages interpolation powered by GenAI to help\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 10, 'page_label': '11'}, page_content='experts understand AI drug design models and verify potential new\\ndesigns.\\n• Graph simulation data . Graph generation can also be applied to infer-\\nence of certain physical simulation data which can be modeled as graph\\ndata structure, such as ocean simulation data [40].\\nMethod. Typical methods include GNN and latent space traversal:\\n• Graph Neural Network (GNN). GNN has been developed to model data\\nthat can be represented as graphs [111]. By extracting graph fea-\\ntures with operations like graph convolution, GNN can be applied to\\na wide range of non-Euclidean data with complex relationships, which\\ncan also benefit some visualization tasks such as structure-aware vi-\\nsualization retrieval [45] and data reconstruction [40]. For example,\\nGNN-Surrogate [40] is proposed to reconstruct ocean simulation data\\nbased on simulation parameters for efficient parameter space explo-\\nration. Particularly, because training an end-to-end model that directly\\nreconstruct the full high-resolution ocean simulation data is expensive,\\nthe authors proposed to construct an intermediate graph representation\\nfor adaptive resolution. The hierarchical graphs are constructed with a\\nseries of operations including edge-weighted graph construction, graph\\nhierarchy generation and hierarchical tree cutting. GNN-Surrogate,\\nwhich is an up-sampling graph generator, first transforms input pa-\\nrameters into latent vector. Then the latent vector is reshaped into\\ninitial graph, which is passed through multiple steps of graph convo-\\nlutions with residual connections. Specifically, graph convolution gen-\\nerates the features of each node by weighted sum of features in the\\nprevious layer for the node and all its neighbors. In chemical data\\ninterpolation, sometimes special latent space traversal algorithm need\\nto be developed to generate desirable intermediate samples, because\\nthe direct linear interpolation assumes that the latent space is flat and\\nEuclidean [112], which may poorly model the complex structure, par-\\nticularly for chemical molecular data. To address this challenge, some\\nresearchers develop special traversal methods [75, 41, 76]. For example,\\nChemoVerse [75] introduces a manifold traversal algorithm. To find a\\npath going through regions of interest, a k-d tree is built based on the\\nJacobian distances of all points of interest and additional user-specified\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 11, 'page_label': '12'}, page_content='constraints. Then A∗ algorithm is combined with Yen’s algorithm [113]\\nto find the shortest path in the k-d tree. Subsequently, data interpola-\\ntion is performed along this path by sampling points at equal interval\\nand decoding the latent vectors into full-fledged molecular structures\\nwith the generation model.\\n3.1.2. Tabular Generation\\nData. GenAI can be used to synthesize surrogate data for subsequent tasks,\\nsuch as privacy protection. Specifically, to protect users’ data, oftentimes\\nmany institutions would not reveal the real data to the public, causing lack\\nof data for domain-specific analysis tasks. Instead, some studies aim at\\ngenerating surrogate data similar to real data which can be freely used to\\ntest downstream tasks such as visualization and query [63, 64, 38, 65, 66, 67].\\nThe data are typically relational data.\\n• Relational data . Relational data is the most basic form of data for\\nvisualization which are often stored in tabular format comprising data\\nitems in rows and multi-dimensional attributes in columns. Surrogate\\ndata generation studies mainly focus on tabular relational data.\\nMethod. A common method for tabular data generation is GAN.\\n• Generative Adversarial Network (GAN) . For example, in recent years,\\nsome researchers attempt to generate relational data similar to real\\ndata with GANs [63, 38, 65, 66]. The architecture of GANs consists\\nof a generator and a discriminator. The adversarial training scheme\\nwhere the generator progressively learn to generate more realistic data\\nthat can deceive the discriminator enables GANs to model the dis-\\ntribution of real data. For example, table-GAN [38] builds upon the\\nbasic deep convolutional GAN (DCGAN) [114] framework and tailor\\nthe generation to tabular data. Specifically, first the tabular records are\\nconverted into square matrix to accommodate convolution operation.\\nIn addition to the original generator and discriminator, table-GAN also\\nincorporates a classifier network which learns the correlation between\\ncategorical labels and other attributes from the table. This serves to\\nmaintain the consistency of values in the generated records. Moreover,\\nbesides the original adversarial loss, the authors design a new informa-\\ntion loss, which measures the first order and second order statistical\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 12, 'page_label': '13'}, page_content='difference between the high-dimensional embedding vectors before the\\nsigmoid function in the discriminator. However, purely GAN-based\\nmethods still leak important features of user data as they are directly\\ntrained on real data. To address this risk, SERD [65] seeks to generate\\nsimilar data while preserving the key privacy information in real data.\\nSpecifically, SERD manages to satisfy the differential privacy guarantee\\nconditions by using fake entities satisfying the same vectorized similar-\\nity constraint of entities in real datasets.\\n3.1.3. Spatial Generation\\nData. GenAI can be used to infer spatial data such as imagery, volumetric\\nor spatial-temporal data.\\n• Imagery data . Some studies apply GenAI to image data inference,\\nincluding emoji images [46], medical images [115], natural images [116,\\n117], etc. The inferred data are used for subsequent tasks like visual\\ninterpolation [46, 118], super-resolution [115], 3D reconstruction [68],\\nobject detection [48] and semantic segmentation [69].\\n• Volumetric data . Volumetric data is a type of data that represents\\ninformation in three-dimensional space, which is widely used in various\\nfields such as biology, geology, and physics. Some studies use GenAI\\nfor volumetric data super-resolution to address the problem of low data\\nquality [70]. Others works focus on volumetric data reconstruction\\nthrough generation model. For example, DeepOrganNet [68] applies\\nGenAI to reconstructing and visualizing high-fidelity 3D organ models\\nbased on input of merely single-view medical images.\\n• Spatial-temporal data. Spatial-temporal data such as flow data is a\\ntype of data that combines both spatial and temporal components. It\\ninvolves information that varies not only in space (location) but also\\nover time. GenAI can be applied to data extrapolation of spatial-\\ntemporal data. For example, Wiewel et al. [71] leverages GenAI to\\nmodel the temporal evolution of fluid flow. Super-resolution can also\\nwork on spatial-temporal data. For example, STNet [47] addresses the\\nspatial-temporal super-resolution of volumetric data.\\nMethod. Spatial data inference typically include VAE, GAN and DRL meth-\\nods.\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 13, 'page_label': '14'}, page_content='• Variational Autoencoder (VAE). VAE [119] is a commonly used gener-\\native method that formulates generation as an autoregressive learning\\nframework. The basic autoencoder architecture consists of an encoder\\nextracting data features into latent representation vectors and a de-\\ncoder reconstructing the data from the latent vectors. To allow GenAI\\nto capture the variability in data, VAE builds on traditional autoen-\\ncoder by modeling latent representation as probablistic instead of a\\nfixed vector. During generation, the decoder sample from the distri-\\nbution in the latent space and synthesize new data. VAE has been\\nexploited for many data inference tasks in visualization such as data\\ninterpolation. For example, Latent Space Cartography [46] trains mul-\\ntiple VAE models with different hyperparameters on 24,000 emoji im-\\nages. Users can explore the latent space of these VAEs and define\\ncustomize semantic axes by selecting samples representing two ends\\nof opposing concepts. Subsequently, linear interpolation is performed\\nat constant intervals along the axis to generate intermediate samples\\nshowing the transitions of visual features of the emoji images.\\n• Generative Adversarial Network (GAN). GAN [120] models the process\\nof generation as an adversarial learning framework, the basic compo-\\nnents of which are the generator and the discriminator. The genera-\\ntor G is designed to generate data that resembles real training data.\\nThe discriminator D is designed to distinguish the data generated by\\nthe generator from the real data. The training alternates between the\\ngenerator and discriminator to optimize a min-max problem with the\\nfollowing objective function. Different from the original GAN, spatial-\\ntemporal adversarial generation requires a spatial-temporal generator\\nand discriminator. For example, STNet [47] builds a ConvLSTM struc-\\nture for discriminator. Specifically, Convolution layers are used to ex-\\ntract spatial features. Then features of adjacent time steps are fed into\\nConvLSTM to evaluate temporal coherence. Global average pooling is\\nused to produce the final single value score for realness.\\n• Disentangled Representation Learning (DRL). DRL with VAEs or GANs\\nhave been applied to visual analytics to identify interpretable dimen-\\nsions interactively [48, 73, 69, 72, 74]. The disentangled dimensions\\ncan be subsequently controlled by users to generate meaningful data\\nfor augmentation. In the general literature of computer vision and\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 14, 'page_label': '15'}, page_content='Concept\\nImage\\nVAE Disentangling\\nDisentangle\\nUnseen Cases\\nTraining Data\\n(a) VATLD (b) SCANViz\\nSemantic Adversarial Learning\\nFigure 3: Data enhancement with disentangled representation learning, such as\\nVATLD [48] and SCANViz [73].\\ncomputer graphics, disentanglement has been an essential technique for\\ncontrollable generation [121, 122, 123]. A commonly used DRL archi-\\ntecture is β-VAE [124]. The objective function of β-VAE is a modifica-\\ntion of the original VAE with an additional β parameter. Experiments\\nshow that better chosen β value (typically < 1) can produce more\\ndisentangled latent representation z. For example, VATLD [48] is a vi-\\nsual analytics system that adapts β-VAE to extract user-interpretable\\nfeatures like colors, background and rotation from low-level features\\nof traffic light images. With such interpretable features encoded in\\nlatent space, users can generate additional training examples in an in-\\nterpretable manner to enhance the traffic light detection model. The\\nDRL scheme distills potentially significant semantic dimensions in la-\\ntent space representation for data summarization and semantic control\\nby users. Particularly, two additional losses are introduced to the orig-\\ninal β-VAE, namely the prediction loss and perceptual loss to ensure\\ngeneration and reconstruction of more realistic traffic light images.\\n3.1.4. Discussion\\nGenAI methods like GANs are not designed to predict the accurate data\\nvalues. Instead, they focus on generating reasonable data based on the given\\ndistribution of the real data. Such generation should not be overclaimed or\\nmisused for tasks that require accurate data features. A specific example is\\n15'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 15, 'page_label': '16'}, page_content='outlier data, which might pose challenges as GenAI methods predominantly\\nconcentrate on learning the overall data distribution. Particularly, generative\\nmodels aim to generate data that closely matches the majority of the training\\ndata. If the outliers are rare and significantly different from the majority of\\nthe data, the generative model may not capture them effectively. Outliers\\ncan be overlooked or underrepresented in the generated samples.\\nDespite being designed to embed data in more disentangled dimensions,\\nthe automatic DRL methods cannot guarantee the resulting dimensions are\\nperfectly interpretable and disentangled. Consequently, the generative mod-\\nels built upon DRL are still largely a black box. For example, the choice of\\nβ in β-VAE still remains largely heuristics. In addition, even though the di-\\nmensions extracted by DRL may be meaningful, it may not encode the visual\\nproperties users intend to explore, thus can potentially limit the customizable\\ndata exploration on users’ part. Some recent work attempts to incorporate\\nmore user interaction to refine DRL through visual interface. For example,\\nDRAVA [72] not only allows the meaning of DRL dimensions to be verified\\nby users, but also enables user refinement. To facilitate user refinement of\\nconcept dimensions, they propose a light weight concept adaptor network on\\ntop of the VAE. The concept adaptor is a multi-class classifier to predict the\\ncorrect grouping of data points along a selected dimensions for semantic clus-\\nterings. However, such interactions are still limited in some aspects. Users\\nmay only be able to verify and refine a small subset of dimensions, leaving\\nmany others unaddressed, because of the lack of overview for the relations\\nbetween data points and different dimensions.\\n3.2. Data Embedding\\nData embedding is an emergent technology that leverages GenAI to em-\\nbed data into visualization images with information steganography. The data\\ncan be recovered losslessly from the visualization images, which are mostly\\n2D imagery data.\\n3.2.1. Spatial Generation\\nData. Spatial generation in data embedding concerns QR code and visual-\\nization image data.\\n• QR Code. QR code data is a special type of data used as the visual\\ncoding scheme of the chart information such as metadata to be em-\\nbedded, which can be processed together with the visualization images\\n16'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 16, 'page_label': '17'}, page_content='Visual Importance\\nInput\\nEncoded Image Decoded Information\\nEncoder Decoder\\n(a) Chartem\\n(b) VisCode (c) InvVis\\nFigure 4: The data embedding pipeline of (a) Chartem [77], (b) VisCode [49] and (c)\\nInvVis [50].\\nwith neural networks. QR code is a reliable coding scheme allowing for\\nerror correction but avoiding artifacts in the encoded image [50, 49].\\n• Visualization Image. Although QR code can encode chart information,\\nit has limited data encoding capacity. For the task of data embedding,\\nvisualization images are the essential medium to carry the encoded data\\nand information. Large number of visualization images are needed to\\ntrain the model. Synthetic data can be used for training, but real-world\\nvisualization image datasets such as VIS30K [125] and MASSVIS [126]\\nhave also been used to increase the generalizability and robustness of\\nthe model. To embed large quantities of underlying data for invertible\\nvisualization, data image that represents raw data produced by data-\\nto-image (DTOI) method can also be used [50].\\nMethod. The data embedding methods include boundary-aware segmentation\\nnetwork (BASNet) and invertible steganography network (ISN) models.\\n• Saliency BASNet . In order to assess the visual quality of the coded\\nvisualization image to ensure it is perceptually identical to the orig-\\ninal image, it is insufficient to measure the pixel-wise mean square\\nerror because it neglects the varying importance of pixels across the\\nvisualization. To address this issue, VisCode [49] proposes a special vi-\\nsual importance network to predict the visual importance map for the\\nchart image. Compared to traditional saliency-based method which\\n17'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 17, 'page_label': '18'}, page_content='are applied to natural images but overlook the unique features of vi-\\nsualization images, the visual importance network can learn from eye-\\nmovement data of real users on chart images. Specifically, the model\\nadopts a BASNet [127] architecture which is originally developed for\\nsalient object detection. The architecture is based on U-Net structure\\nwith residual blocks inspired by ResNet. The loss function combines\\nBCEWIthLogits loss with structural similarity index (SSIM) to balance\\nbetween segmentation accuracy and structural information.\\n• Encoder-decoder ISN model. To encode and decode the visualization\\nimage, QR code image and data image in one unified model, InvVis [50]\\nintroduces a concealing and revealing network. The concealing network\\nand revealing network both consist of two major parts: feature fusion\\nblock (FFB) and invertible steganography network (ISN). FFB is de-\\nsigned to blend the features of data image and QR code image into\\nvisualization image while keeping minimal visual distortion. Specif-\\nically, FFB comprises four dense blocks and three common convolu-\\ntional blocks. Dense blocks [128] are a special type of convolutional\\nneural network architecture which contain multiple layer with dense\\nconnection (each layer is connected to all the preceding layers). Next,\\nthe ISN adds the invertible 1 × 1 convolution to the invertible neural\\nnetwork structure [129], which consist of several affine coupling layers.\\nThe authors also proposed using discrete wavelet transform (DWT)\\nbetween the FFB and ISN to reduce texture-copying artifacts.\\n3.2.2. Discussion\\nCurrently, the evaluation of the quality of data restoration is only lim-\\nited to the data image, using generic metric such as root mean square error\\n(RMSE). However, such pixel-wise metrics cannot fully reflect the accuracy\\nof data restoration due to the absence of original data in the evaluation. In\\naddition, the capacity of visualization image for data embedding is not in-\\nfinite. Specifically, there can be an apparent trade-off between embedding\\ncapacity and image quality. To maintain image quality above certain thresh-\\nold, it becomes more challenging to recover large amounts of data. To ad-\\ndress this concern, more evaluation in practical scenario regarding precision\\nrequirements for the original data need to be conducted.\\n18'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 18, 'page_label': '19'}, page_content='4. Visual Mapping Generation\\nFor non-expert users, it is difficult to make appropriate visualization from\\ndata on their own for data analysis. GenAI plays an essential role in visual\\nmapping synthesis for automatic visualization generation.\\n4.1. Visual Primitives Generation\\nThe fundamental task of visual mapping generation is generating charts\\nwith the basic visual marks or visual primitives.\\n4.1.1. Sequence Generation\\nData. Sequence generation is often applied to visual mapping in different vi-\\nsualization grammars including concrete programming language and abstract\\ncode.\\n• Visualization grammar. Generative AI can be applied to generate vi-\\nsualization code in different grammars based on input data. For ex-\\nample, Vega-Lite code is a commonly used declarative visualization\\nlanguage [29, 130]. Data2Vis [29] treats generation of visual encoding\\nas a sequence-to-sequence generation task which translate strings de-\\nscribing columns of data tables into Vega-Lite code sequences. Other\\nonline repositories such as Plotly also provides codes in other program-\\nming languages such as python. In addition, some studies generate\\nabstract code instead of specific programming code. For example, Ta-\\nble2Charts [30], the authors define a more abstract chart template lan-\\nguage including the essential visual elements and a set of grammar that\\nsummarizes the possible actions in the process of chart creation.\\nMethod. Sequence generation used in visual mapping typically includes RNN,\\nDeep Q Network, NL2VIS Transformer and the latest LLMs.\\n• RNN-based Code Sequence Generation . Some studies formulate the\\nproblem of visualization code generation as sequence-to-sequence gener-\\nation. For example, Data2Vis [29] translates strings describing columns\\nof data tables into Vega-Lite code sequences. For this task, the authors\\ntake inspiration from machine translation and adopt a encode-decoder\\narchitecture based on recurrent neural network model. Specifically, for\\nthe decoder, they construct a two-layer bidirectional RNN; for the de-\\ncoder, another two-layer RNN is used to predict the next token in the\\n19'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 19, 'page_label': '20'}, page_content='code sequence. Both the encoder and decoder leverage the Long Short\\nTerm Memory (LSTM) structure to enhance the model’s ability to deal\\nwith longer sequence.\\n• Deep Q network for encoding action prediction . Some studies consider\\nthe task of visualization generation as the generation of abstract action\\ntokens deciding key features of the charts, including data queries which\\nselect particular fields in data table and design choices which specify\\nvisual encoding operation. In this light, visualization generation can\\nbe formulated as an action prediction task which can be solved by\\nDeep Q Network (DQN). For example, Table2Charts [30] develops a\\nsimple chart template language describing some essential actions for\\ngeneration of six types of charts from data tables. According to this\\ntemplate, the authors construct a DQN for action prediction with a\\ncustomized CopyNet architecture [131]. This network takes all the\\ndata fields and prefix action sequence as input and generate the next\\naction token with a Gated Recurrent Unit (GRU) based RNN structure.\\nIn additon, to address the exposure bias problem with the previous\\nteacher forcing training scheme which only learns the ground truth user\\ngenerated results, Table2Charts adopts the search sampling approach of\\nreinforcement learning to close the gap between training and inference.\\n• Natural language to visualization models . The Natural language to\\nVisualization (NL2VIS) [78, 31, 79, 80] task can be formulated as fol-\\nlows: Given a natural language query (NL) over a dataset or relational\\ndatabase (D), the goal is to generate a visualization query ( e.g. Vega-\\nLite) that is equivalent in meaning, valid for the specifiedD, and, when\\nexecuted, will return a rendered visualization ( V IS) result that aligns\\nwith the user’s intent. For example, ADVISor [78] trained two separate\\nneural networks to provide the NL2VIS functionality. Broadly, ADVI-\\nSor’s pipeline can be divided into two steps: (1) the NL2SQL step,\\nand (2) the rule-based visualization generation step. ADVISor takes as\\ninput a NL question and data attributes associated with the datasets.\\nNext, it first utilizes a BERT-based neural network to generate a vec-\\ntor representation ( q) of the NL question and a corresponding header\\nvector. Subsequently, the system utilizes an Aggregation network to\\ndeduce aggregate operators and a Data network to determine attribute\\nselection and filtering conditions. Upon completing these steps, AD-\\n20'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 20, 'page_label': '21'}, page_content='VISor first queries the dataset. It then maps the query results to a\\nvisualization based on a rule-based visualization algorithm The neural\\nnetwork modules within ADVISor are specifically trained to extract\\nfragments of SQL queries from the given NL query, which indicates\\nthat ADVISor is not an end-to-end NL2VIS solution. On the con-\\ntrary, ncNet [31] is an end-to-end NL2VIS method based on the Trans-\\nformer [132] architecture. ncNet is trained on the first large-scale cross\\ndomain NL2VIS dataset nvBench [79]. ncNet utilizes a Transformer-\\nbased encoder-decoder framework, with both the encoder and decoder\\ncomprising self-attention blocks. This system accepts a NL query, a\\ndataset, and an optional chart template as inputs. ncNet processes\\nthese inputs into embeddings and finally generates a flattened visual-\\nization query through an auto-regressive mechanism. Additionally, nc-\\nNet incorporates a visualization-aware decoding strategy, which allows\\nfor the generation of the final visualization query, with visualization-\\nspecific knowledge.\\n• Large language model for visualization code generation. Recently, some\\nresearchers realize the limitation of previous GenAI methods which\\nonly focus on a particular type of visualization code like Vega-Lite. To\\nimprove the flexibility of visualization code generation, some studies\\npropose using large language model for more robust generation [32, 81,\\n83, 84, 85]. For example, LIDA [32] presents a pipeline called VIS-\\nGENERATOR for AI generation of grammar-agnostic visualizations\\nconnecting data tables to generated visualizations with multiple steps.\\nThe VISGENERATOR consists of three sub-modules: code scaffold\\nconstructor, code generator and code executor. The code scaffold con-\\nstructor generates code that imports language-specific dependencies\\nlike Matplotlib and constructs the empty function stub. Then, in code\\ngenerator, taking as input the dataset summary and visualization goal,\\nLLM is used in the fill-in-the-middle mode [133] to generate concrete\\nvisualization code of the given programming language. Finally, in the\\ncode executor, some filtering mechanisms such as self-consistency [134]\\nand correctness probabilities [135] are incorporated to reduce errors. In\\nthe third step of LIDA, taking as input the dataset summary and visu-\\nalization goal, LLM is used in the fill-in-the-middle mode [133] to gen-\\nerate concrete visualization code of different programming languages.\\nAnother recent study, LLM4Vis [81] proposes leveraging the in-context\\n21'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 21, 'page_label': '22'}, page_content='learning ability of large language models to perform few-shot and zero-\\nshot generation for the same design choice task as in VizML [39]. The\\nkey contribution of this method compared to previous supervised learn-\\ning is that it reduces the need for large corpus of data-visualization pair\\ntraining data and provides more explainable generation. The genera-\\ntion algorithm is retrieval-augmented with demonstration examples.\\nFirst, data feature description is generated to enable GPT to take tab-\\nular dataset as input. Specifically, similar to VizML, with feature en-\\ngineering as many as 120 single-column features and 80 cross-column\\nfeatures are extracted to represent the input dataset. These features\\nare then serialized by TabLLM method [136], which utilizes a prompt\\nto instruct ChatGPT to generate text description that elaborate on\\nthe feature values for each attribute. Next, the demonstration exam-\\nples are selected from the training corpus by similarity retrieval based\\non feature description, which can fit into token limit of LLM without\\nconsidering irrelevant samples. Subsequently, an iterative explanation\\ngeneration bootstrapping module prompts LLM to not only predict the\\ncorrect visual design choices but also generate explanation. Finally, all\\nthe relevant demonstration examples along with the explanation are\\nfed into LLM to optimize in-context learning for generation of appro-\\npriate design choices for the input data. Recently some researchers also\\nleverage LLM to refine the colormap [82].\\n4.1.2. Tabular Generation\\nData. The visual mapping can also be simplified as predicting some tabular\\nattributes such as design choices.\\n• Design choices. Instead of directly generating the visualization images,\\nsome studies focus on generating the most important design choices for\\nthe specifications of visualization, which is represented as tabular data\\nwith each attribute denoting one design dimension.\\nMethod. The tabular generation methods of design choices include fully\\nconnected neural network for direct prediction and design parameter enu-\\nmeration with AI scoring.\\n• Fully connected neural network can be combined with feature engineer-\\ning of data, casting the generation problem into a prediction task. For\\n22'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 22, 'page_label': '23'}, page_content='(a) Data2Vis (b) DeepEye\\nFigure 5: Examples of visual primitive generation. (a) Data2Vis [29] adopts RNN-based\\nsequence generation method. (b) DeepEye [17] combine design parameter enumeration\\nwith AI scoring.\\nexample, VizML [39] builds a feed-forward neural network based on\\n841 dataset-level features extracted from input data table. The model\\npredicts five design parameters of the appropriate visualizations with\\nmulti-head output layer, on both the encoding level and visualization\\nlevel. For example, to generate the visualization type, one of the predic-\\ntion head outputs a 6-class prediction scores for chart types including\\nScatter, Line, Bar, Box, Histogram, Pie .\\n• Design parameters enumeration with AI scoring . Some studies ap-\\nproach the generation problem from a different angle, using AI as\\nthe judge of the candidate generation results. For example, Deep-\\nEye [17, 86, 137, 138, 139] combines rule-based generation with data-\\ndriven machine learning to classify and rank meaningful visualizations.\\nSpecifically, based on a collected corpus of real world visualization\\ncases, a classification model and a ranking model are trained. When\\nusers input a new dataset to visualize, the system first generates candi-\\ndate visualizations by enumerating valid combinations of transforma-\\ntions and visual encoding in a pre-deifined search space. The classifica-\\ntion model then determines whether a visualization candidate is mean-\\ningful. Subsequently, the ranking model sort the remaining meaningful\\nvisualization and recommend to users. In case the machine learning\\nmodels do not yield satisfactory results, DeepEye also supports incor-\\nporation of expert-designed domain rules. The recommendations of\\n23'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 23, 'page_label': '24'}, page_content='the machine learning model and the rule-based method can also be\\ncombined with a linear model. Similarly, Text-to-viz [18] adopt a hy-\\nbrid method combining template-based enumeration with AI-powered\\ncomprehension of user inputs and relevance ranking.\\n4.1.3. Spatial Generation\\nData. Spatial generation in visual mapping mainly concerns 2D sketch, den-\\nsity map and volumetric data.\\n• Sketch. Sketch is a special type of 2D image data with simple informa-\\ntion of drawing trajectory. It has attracted much research interest in\\nthe broader field of generative AI [140] because it allows designers to\\nfollow their familiar workflow of prototype design and allows them to\\nhave spatial control over the generated results. Recently, some stud-\\nies explore using sketch as a medium to facilitate fast prototyping of\\nvisualizations with sketch-to-vis generative AI [51, 141].\\n• Density map . Density maps are a special type of visualization that\\ncan vary dynamically depending on the time. Traditionally, the spa-\\ntial temporal data collected for density map visualization is discrete\\nand static. For smoother transition of density maps at different dis-\\ncrete observation times, some researchers propose to utilize generative\\nAI [52].\\n• Volumetric data. Apart from the enhancement of volumetric data as\\nintroduced in Section 3.1.3, some studies also leverage GenAI methods\\nfor the rendering of such data [87, 88].\\nMethod. Spatial generation methods for visual mapping mainly includes\\nFaster R-CNN with validation model, GAN-based density map generation\\nand GAN-based volume rendering.\\n• Faster R-CNN for sketch recognition with validation model . For the\\ntask of sketch to dashboard generation, latest AI-driven approach com-\\nbines AI-powered chart recognition with rendering algorithms such as\\ncolor palette recommendation and layout optimization. To detect the\\ncharts and the basic visual encoding features, LADV [51] first applies a\\nFaster R-CNN network [142] which exploits region proposal to achieve\\n24'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 24, 'page_label': '25'}, page_content='Figure 6: Examples of LADV [51] sketch to dashboard generation.\\nefficient object detection. Specifically, Faster R-CNN further acceler-\\nates Fast R-CNN by computing the region proposal with a deep CNN-\\nbased region proposal network which can share weights with the subse-\\nquent object detection network. In addition, to adapt Faster R-CNN to\\nchart recognition, LADV [51] further incorporates a validation model\\nto filter chart candidates, which trains a logistic regression for each\\nchart type to capture the location and size.\\n• GAN-based density generative model . To generate dynamic density\\nmaps, density generative model is developed. For example, Gener-\\nativeMap [52] adopts a GAN-based generative model. Specifically,\\nthe authors first generate synthetic training data using Perlin noise.\\nThen, they adapt Bidirectional Generative Adversarial Network (Bi-\\nGAN) [143] to the density generation with enlarged convolution ker-\\nnels and blocks inspired by ResNet [144] to enable processing of larger\\nfields. Poisson blending is also used to make the density change more\\nnatural.\\n• GAN-based Volume rendering. Some researchers adopt GenAI for the\\nrendering of 3D volumetric data [87, 88, 89, 90]. For example, Berger\\n25'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 25, 'page_label': '26'}, page_content='Figure 7: Knowledge-graph-empowered visual mapping generation in KG4VIS [42].\\net al. [87] proposed a framework that combines two GANs for the task,\\nnamely opacity GAN and opacity-to-color translation GAN. Such ap-\\nproach breaks down the more difficult task of rendering volumes com-\\npared to the image generation task of the original GAN. Specifically,\\nthe opacity GAN learns to generate an opacity image given the input\\nof viewpoint and opacity transfer function, which captures the shape,\\nsilhouette, and opacity. The second GAN translates the combined in-\\nputs of the viewpoint, the opacity transfer function’s representation in\\nthe latent space, color transfer function, as well as the opacity image\\ngenerated by the first GAN into the final rendered image.\\n4.1.4. Graph Generation\\nData. The graph generation in visual mapping mainly involves knowledge\\ngraphs.\\n• Knowledge graph. Knowledge graph is a graph structure representation\\nof knowledge that captures the relationships between entities in a par-\\nticular domain, which is composed of nodes representing entities and\\nedges representing relationships between these entities. Some studies\\nalso leverage knowledge graphs to support more explainable generation\\nof visualization [42].\\nMethod. The method for knowledge graph enhanced visual mapping genera-\\ntion is knowledge graph embedding.\\n• Knowledge-graph embedding. KG4VIS [42] applies knowledge graph to\\nthe design choice generation task of VizML [39]. The knowledge graph\\n26'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 26, 'page_label': '27'}, page_content='is constructed with entities representing data features, data columns\\nand visualization design choices, and the relations between them. Then,\\nKG4Vis employs knowledge graph embedding method TransE [145] to\\nlearn neural embedding representations of all the entities and relations.\\nWhen generating design choices for new input data, the model encode\\nthe input data in the embedding space and evaluate the scores of differ-\\nent candidate visual encoding rules with arithmetic operation adding\\nrelation vectors to entity vectors and measuring the distance from de-\\nsign choice vectors.\\n4.1.5. Discussion\\nLimitations of the rules and the training data . The training data\\ncan be a significant limiting factor for visual primitive generation. For ex-\\nample, Data2Vis [29] only trains the model on four major chart types, which\\nsignificantly limits the scope and diversity of the generation. In addition,\\nsome hybrid methods combining rule-based components with GenAI may be\\nlimited by the rules that are not comprehensive enough. For example, the\\nchart template defined in Table2Charts only [30] include the most basic vi-\\nsual mark and direct data field reference without even considering some basic\\noperation like aggregation.\\nGeneration of visualization images vs. natural images . Image\\ngeneration has been a heavily researched topic in the general field of AI\\nand computer vision. However, the generation of visualization images rarely\\nadopts a fully end-to-end method without rule-based constraints. This is\\npartly because of the difference between visualization images and natural\\nimages. Especially, compared to the irregular and complex visual features in\\nnatural images, the dominance of regular shapes rigid structures in visualiza-\\ntion images makes it difficult for GenAI to accurately maintain. The reason is\\nAI models are inherently stochastic and treats the image features as a whole\\nwith little knowledge of the structural constraints. To address this issue,\\nfuture work may take inspiration from some recent computer vision studies\\nthat seek to incorporate structural information in image processing [146].\\n3D diffusion . Recently, inspired by the success of text-to-image gen-\\neration diffusion model, some researchers seek to develop diffusion-based\\ntext-to-3D [16, 147] models to allow more intuitive interactive control of the\\ngeneration, including natural language guided generation and editing. Such\\ntechnology can potentially benefit generation of visualization, especially in\\nproviding multi-modal control.\\n27'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 27, 'page_label': '28'}, page_content='NL2VIS challenges. Unquestionably, LLMs offer a complementary di-\\nmension to the NL2VIS system. However, the integration of LLMs into\\nNL2VIS through prompt engineering presents certain limitations. First, re-\\nlying solely on a simple prompting-based method may not effectively enable\\nLLMs to fully comprehend the intricacies of the NL2VIS task. This ap-\\nproach could limit the model’s ability to accurately interpret and respond to\\nmore complex visualization queries, potentially overlooking nuanced aspects\\nof data visualization requirements. Second, current LLMs-based NL2VIS so-\\nlutions often do not incorporate specific domain knowledge from the fields\\nof visualization and data analysis into the LLMs. This absence of domain-\\nspecific integration can result in suboptimal performance, as the models may\\nnot leverage the rich contextual and technical knowledge necessary for pro-\\nducing highly accurate and relevant visualizations. Furthermore, current\\nLLMs-based NL2VIS solutions struggle to guarantee the semantic correct-\\nness of generated visualizations, which is crucial for accurate data represen-\\ntation and interpretation. Additionally, these systems often face challenges\\nin interactively fine-tuning results based on user feedback, a key aspect for\\nachieving user-centered visualization design.\\nGiven these challenges, future research directions may include: 1) Devel-\\noping methods to integrate specific domain knowledge related to visualization\\nand data analysis into LLMs. This could involve training models on special-\\nized datasets or incorporating expert systems that guide the LLMs in under-\\nstanding domain-specific visualization tasks, knowledge, and requirements.\\n2) Ensuring the semantic correctness of visualizations generated by LLMs,\\nwhich can explore validation strategies that automatically check and confirm\\nthe accuracy and relevance of visualizations with respect to the underlying\\ndata. 3) Enhancing the interactivity of LLMs-based NL2VIS systems by in-\\ncorporating more robust and flexible user feedback loops. For example, it\\ncan explore how to incorporate other modalities ( e.g.clicks) user feedback.\\n4) Investigating hybrid models that combine the strengths of LLMs with\\ntraditional data visualization techniques. Such hybrid systems could lever-\\nage the natural language understanding capabilities of LLMs while ensuring\\nadherence to best practices in data visualization.\\n28'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 28, 'page_label': '29'}, page_content='5. Stylization\\n5.1. Style Transfer\\nStyle broadly refers to the visual or aesthetic characteristics of an image,\\nwhich oftentimes involves some global or overall features that can affect the\\nviewers’ general appreciation. More concretely, it can involve many aspects\\nsuch as color, texture and layout. Some style transfer studies focus on the\\noverall style while others focus on particular aspects like color.\\nCreating visualization in design practice relies on existing examples from\\nthe internet, offering inspirational visual materials toward style. This has\\nspawned extensive research on transferring style attributes from these exist-\\ning materials to facilitate the development of intended visualization designs.\\nTo transfer overall visual style, some research [148, 91, 54, 51] summarize\\nstyle as a template and migrate these graphical attributes disentangled from\\nthe content to restyle new data source.\\n5.1.1. Spatial Generation\\nData. Researchers frequently employ visualizations in image format to train\\nGenAI models, which are designed to extract specific attributes for transfer\\ntasks.\\n• Imagery data. End-to-end GenAI models consume a substantial amount\\nof visual images, primarily sourced from the internet or synthesized us-\\ning tools like D3 [149]. Examples include MassVis [126], InfoVIF [94],\\nand Visually29K [150].\\n• Task-oriented Labeling. General image datasets often lack paired at-\\ntributes, necessitating an extraction stage for labeling task-oriented\\nattributes. For instance, the color transfer task requires extracting the\\ncolor map for subsequent training. Achieving the overall transfer ne-\\ncessitates considering multiple attributes to describe a comprehensive\\nvisual representation.\\nMethod. The methods include color transfer and hybrid attribute transfer.\\n• Color transfer. As one of the most important visual channels in data\\nvisualization, generations of scholars have investigated the issue of color\\ntransfer [53, 151]. To extract the color at multiple scales in the Lab\\n29'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 29, 'page_label': '30'}, page_content='histograms, Yuan et al. [53] employed a neural network featuring an\\natrous spatial pyramid structure, predicting the colormap of visualiza-\\ntion and supporting discrete and continuous formats. Similarly, Huang\\net al. [91] approached the problem of color extraction in the foreground\\nof visualization with Faster-RCNN. With the reference example as the\\nnatural image, some research [152, 92] generates a harmonious palette\\nfor a visualization based on color detection and extraction from im-\\nages. For instance, Liu et al. [92] distinguished the salient subject in\\nthe image to extract the color with high visual importance that aligns\\nwith human perception.\\n• Hybrid attribute transfer with Siamese Network . Transferring multiple\\nattributes from an example to the current design involves recognizing\\nthe content of the example and adapting it to the current design [54].\\nLu et al. [94] curated a comprehensive infographic dataset and proposed\\na model based on YOLO to identify various visual elements, including\\ntext, icons, indices, and arrows. To maintain style consistency between\\nthe example and the current design, Vistylist [54] employs a Siamese\\nNeural Network [153]. This network embeds visual elements into a\\n256-dimensional vector and compares the Euclidean distance between\\npairs. However, assessing the priority of different visual elements in a\\nvisualization poses a challenge. Huang et al. [91] proposed a restyling\\napproach with an attention mechanism to weigh different visual prop-\\nerties for input visualizations. Accurate recognition of the example’s\\ncontent and the ability to reproduce it enable hybrid attribute transfer.\\nThis not only maintains style consistency but also generates a design\\ntailored to the provided content. For example, when the template time-\\nline has limited space, Chen et al. generated an extended timeline with\\na similar visual style to the template [93].\\n5.1.2. Graph Generation\\nData. When considering the structure and imagery format of graphs, the\\ndata fed into GenAI models falls into two primary categories.\\n• Graph Structure Data. This includes nodes and edges in a graph, with\\nthe node feature vector and adjacency vector utilized to describe the\\ngraph structure that can be recognized by GenAI models. Various\\nembedding techniques, such as node2vec [154], have been proposed to\\nencode node information.\\n30'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 30, 'page_label': '31'}, page_content='• Imagery data. Given that many graphs are in pixel format, GenAI\\nmodels also process such data to extract low-level features for training.\\nMethod. The method mainly includes graph layout transfer.\\n• Graph layout transfer. Some researchers use generative AI for graph\\nlayout transfer [43, 95, 44] which seeks to learn the style of graph layout\\nfrom examples. For example, to help users intuitively produce diverse\\ngraph layouts from a given set of examples without mannually tweaking\\nlayout parameters, Kwon and Ma [43] designed a GenAI method based\\non encoder-decoder architecture combined with a 2D latent space. The\\nmodel generally follows a VAE framework, taking as input graph lay-\\nout features represented as relative pairwise distances and adjacency\\nmatrix. Then, GNN is used to compute graph-level representation of\\nlayout in both the encoder and decoder. In addition, the latent repre-\\nsentation of layout zL is combined with node-level features through a\\nfusion layer. The latent space is also visualized in a 2D map to facilitate\\nexploration.\\n5.1.3. Discussion\\nDomain knowledge. Different visualizations have specific requirements\\nand constraints for style transfer, often involving the integration of domain\\nknowledge. For the categorical data, Zheng et al. [155] introduced a method\\nto sample dominant colors from the image to preserve color discriminability,\\neffectively enhancing and aiding in the interpretation of the patterns present.\\nAs for scientific visualization like terrain maps, domain-specific elements in-\\ncluding continuity of elevation and hypsometric tints in aerial perspective are\\ninjected into the transfer process to convey the necessary information in a\\nscientifically accurate manner [156].\\nReference image. Furthermore, it is worth mentioning that the refer-\\nence images used for style transfer are not limited to visualization examples\\nalone. Significant works [157, 53, 91] have explored using natural images as\\nreference sources for style transfer, taping into the inherent visual appeal\\nand cognitive stimulation provided by natural images. Recent research has\\nshown that natural images can also serve as an adorable source to stimulate\\nhuman intelligence [92, 156].\\n31'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 31, 'page_label': '32'}, page_content='5.2. Chart Embellishment\\nVisually embellished visualization showcases its memorability and expres-\\nsiveness [158, 159, 160]. The creation of visually appealing and informative\\ngraphical enhancements necessitates design expertise. Fortunately, the ad-\\nvent of generative AI offers a strong framework to streamline the design\\nprocess, particularly for pictorial visualization and glyph generation.\\n5.2.1. Spatial Generation\\nData. The data that is concerned in GenAI for pictorial visualization is\\nimages with semantic correlation with the charts.\\n• Image as pictorial embellishment . In visualization, non-visualization\\nimages such as natural images or artistic images can be used as pic-\\ntorial embellishments to enhance the visual appeal of the data being\\npresented. Images can be added to charts, graphs, and other visualiza-\\ntions to provide additional context and meaning to the data [161, 162].\\nFor example, an image of a product can be added to a sales chart to\\nhelp viewers understand which product is being represented by each\\ndata point. Similarly, an image of a city skyline can be added to a map\\nto help viewers identify the location being represented.\\nMethod. The GenAI method for pictorial visualization mainly include Stable\\ndiffusion based techniques.\\n• Stable diffusion . Pictorial visualization plays a crucial role in seam-\\nlessly integrating semantic context into a chart. Instead of relying on\\npre-existing graphical elements sourced online and adjusting them to fit\\nthe desired visualization, generative AI takes an end-to-end approach\\nby incorporating users’ prompts. Recent advancements [32, 22, 96] in\\nthis field have led to the automation of the creation process through\\nthe transformation of chart components into semantic-related objects.\\nFor example, viz2viz [96] develops specific pipelines for generating var-\\nious types of visualizations. The way they leverage textual prompts as\\ninput surpasses the previous language-oriented creation tools limited\\nby a predetermined set of entities [18, 34], providing a robust seman-\\ntic recognition to arbitrary user input. Furthermore, Xiao et al. [22]\\nproposed a unified approach that classifies visual representations into\\n32'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 32, 'page_label': '33'}, page_content='foreground and background. This approach provides users with an in-\\nterface to select the intended data mask and incorporates an evaluation\\nmodule to assess the visual distortion in the generated visualization.\\n5.2.2. Discussion\\nFlexibility and controllability. When comparing generative AI with\\nthe traditional methods of designing visual elements from scratch or retriev-\\ning relevant resources, generative AI offers several distinct advantages. It\\nprovides inspiration and eliminates the tedious and time-consuming process\\nof adjusting elements to fit the data. Moreover, it empowers users by allowing\\nthem to personalize the style of the generated results with a simple utter-\\nance, saving significant time that would otherwise be expended on searching\\nfor appropriate resources across the vast expanse of the internet. Recent\\nadvancements in generative models, particularly in the field of text-to-image\\nmodels, have achieved remarkable breakthroughs in enhancing control over\\nthe generated output, including layout [163], text content [164], vector graph-\\nics [165], etc. However, apart from general control, it is imperative to priori-\\ntize data integrity throughout the generation process, as visualizations serve\\nto convey data patterns faithfully.\\n5.3. Textual Annotation\\nTextual annotation plays a pivotal role in the realm of data visualiza-\\ntion, enhancing human interpretation, interaction, and comprehension. Text\\nannotations incorporated in the visualization guide users’ interactions with\\nthe artifact [166], explain what the data means [167], and prioritize certain\\ninterpretations of the data [168]. In this way, annotations act as cognitive\\naids, enhancing the overall user experience.\\n5.3.1. Sequence Generation\\nData. The data in this task mainly includes text-integrated data and and\\ncontextually interpreted data.\\n• Text-integrated data. Text-integrated data refers to datasets that in-\\nherently come with accompanying textual content, such as articles,\\nreports, or storytelling contexts. In these instances, the data is inter-\\ntwined with text, forming a cohesive narrative or explanatory structure.\\nTextual annotations in such datasets help make complex data more ac-\\ncessible and understandable to the audience, guide their attention to\\n33'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 33, 'page_label': '34'}, page_content='trends that align with textual content, and provide a richer and more\\nnuanced understanding of the data and its relevance to the text.\\n• Contextually interpreted data. For the datasets that lack accompany-\\ning textual narratives or descriptions, the challenge lies in analyzing\\nthe data to extract relevant insights and generate meaningful textual\\nannotations that align with the visual elements. Effective textual an-\\nnotations act as a bridge between complex datasets and audiences,\\nproviding a layer of interpretation that the raw data lacks on its own.\\nMethod. To further automate the annotation generation process, researchers\\nhave developed innovative approaches, mainly through sequence generation.\\n• Deep learning detection with template-based generation. Lai et al. [169]\\nemploy a Mask R-CNN model to identify and extract visual elements\\nin the target visualizations, along with their visual properties. The\\ndescriptive sentence is displayed beside the described focal areas as\\nannotations. AutoCaption [35], a deep learning-powered scheme, gen-\\nerates captions for information charts by learning the noteworthy fea-\\ntures aligned with human perception and leveraging one-dimensional\\nresidual neural networks to analyze relationships between visualization\\nelements. These advances in automated annotation generation hold\\npromise for applications in education and data overviews. Researchers\\nin other areas such as NLP also study the problem of chart summa-\\nrization [170].\\n• Large Language Models (LLMs). Recent developments in large lan-\\nguage models (LLMs) have opened up new possibilities in generating\\nengaging captions for generic data visualizations. Liew and Mueller [33]\\napply LLMs to produce descriptive captions for generic data visualiza-\\ntions like a scatterplot. Ko et al. [97] introduce a large language model\\n(LLM) framework to generate rich and diverse natural language (NL)\\ndatasets using only Vega-Lite specifications as input. This underscores\\nthe growing role of prompt engineering techniques in shaping the fu-\\nture of annotation techniques and prompts a reevaluation of research\\ndirections.\\n34'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 34, 'page_label': '35'}, page_content='Figure 8: Annotated example cases of AutoCaption [35]. The chart type includes the bar\\nchart, the scatterplot, and the line chart.\\n5.3.2. Discussion\\nChallenges for textual annotation. Researchers have consistently un-\\nderscored the significance of annotations in visualization design, both at the\\nvisual memory level [126] and the cognitive level [171]. These studies reaffirm\\nthat annotations play an indispensable role in enhancing comprehension and\\nretention of visual information [59]. In the realm of annotation for visualiza-\\ntion, it is imperative to address three significant aspects for future research\\nand development: mitigating occlusion problems, harnessing advanced tech-\\nniques for automation, and enriching visual design. Firstly, a persistent\\nchallenge is the occlusion problem, wherein the annotations block the charts.\\nDespite considerable efforts to improve the layout of annotated charts, this\\nissue continues to hamper the effectiveness of annotations. Therefore, more\\nsuitable design space should be surveyed and innovative strategies for annota-\\ntion placement should be considered. Secondly, recent advancements in deep\\nlearning, exemplified by large language models and diffusion models, offer\\nremarkable potential for improving the efficiency of the automatic annota-\\ntion process. These models can consider contextual information to produce\\nannotations that are contextually relevant and strategically placed, allevi-\\nating the burden on users. Moreover, it is essential to enhance the design\\n35'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 35, 'page_label': '36'}, page_content='of annotations by incorporating rich visual cues, which aid in highlighting\\npatterns and facilitating deeper understanding. Researchers should ensure\\nthe visually engaging annotations complement the overall visual composition\\nand improve user engagement.\\n5.4. Timeline & Story\\nBoth timelines and storylines use lines to describe a sequence of events.\\nSpecifically, in a storyline visualization, each role is represented as a line.\\n5.4.1. Spatial Generation\\nData. GenAI for story generation commonly concerns about optimizing the\\nspatial layout of different story components.\\n• Storyline Layout Data . To training GenAI models to optimize the\\nstoryline layouts, it is necessary to generate a large number of high-\\nquality storyline images. In PlotThread [55], Tang et al. construct a\\ndataset of automatically-generated storyline layout pairs, consisting of\\na original layout and a optimized layout simulated by the optimization\\nmodel with randomly-selected constraints.\\nMethod. The method includes\\n• Reinforcement Learning for image-based storyline . To boost a collab-\\norative design of storylines between AI and designers, Tang et al. [55]\\nfurther proposed a reinforcement learning framework and introduced\\nan authoring tool, PlotThread, that integrates an AI agent for efficient\\nexploration and flexible customization. The goal of this is to imitate\\nand improve users’ intermediate results when optimizing storyling lines.\\nTherefore, it is necessary to understand the states of different layouts,\\ndecompose a storyline into a sequence of interactive actions, and pro-\\nvide subsequent actions for layout optimization. They also define the\\nreward as the similarity between the user layout and generated inter-\\nmediate layout to improve the agent’s prediction ability.\\n5.4.2. Graph Generation\\nData. Researchers seek to understand visualization content automatically\\nfrom the image input, which can be mainly categorized into raster image\\nand vector image.\\n36'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 36, 'page_label': '37'}, page_content='• SVG. A recent attempt is to apply GenAI technologies to vector charts\\ndue to the need of motioning images [100]. They use charts of SVG\\nformat to extract and model corresponding structural information be-\\ntween graphical elements without high computation cost.\\nMethod.\\n• Graph Nerual Network for structured dynamic charts . Ling et al. [100]\\npresented an automated method that transforms static charts into dy-\\nnamic live charts for more effective communication and expressive pre-\\nsentation. To overcome the difficulty of generating dynamic live charts\\nfrom static vector-based SVG, this study proposes using GNN for un-\\nderstanding chart and recovering data and visual encodings. Specifi-\\ncally, they first transform raw SVG into graph by a graph construction\\nalgorithm which extracts 5-dimensional node features including element\\ntype, node color, fill color, stroke color and stroke width; then it builds\\ntwo types of edges including stroke-wise edges and element-wise edges.\\nThe constructed graph is then fed into two GNN-based encoder, each\\ndesigned for one type of edges, to generate graph representations, which\\nare subsequently passed to multi-layer perceptron to classify each graph\\nelement.\\n5.4.3. Sequence Generation\\nData. To generate a complete story, most studies generate a sequence of data\\nfacts and ensemble them into a complete data story.\\n• Relational data . As the most basic format of visualization data, re-\\nlational data is also a popular input of automatic story generation.\\nGenAI are applied to generate textual descriptions for data tables [100]\\nand construct the links between visuals and narrations through data\\ntable and word inputs [98].\\n• Time Series Data . Time series data is a widely used data type for a\\nvariety of visual analysis tasks, ranging from visual question answering\\nto free exploration. The traditional tools are mostly designed for single-\\nstep guidance while GenAIs provide opportunities to build a continuous\\nexploratory visual analysis process by extracting coherent data insights.\\nMethod.\\n37'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 37, 'page_label': '38'}, page_content='• Large Language Model . Ling et al. [100]’s work also leveraged large\\nlanguage models to create animated visuals and audio narrations, in-\\ncluding narration with contextual information, narration with insights\\nand narration rephrasing. To further enhance the interplay between\\nvisual animation and narration in data videos, Data Player [98] applies\\nlarge language models to establish semantic connections between text\\nand visualization and then recommends suitable animation presets with\\ndomain-knowledge constraints. Specifically, the authors design special\\nprompt engineering with few-shot pre-defined examples illustrating how\\nto output semantic links sequence provided the input of both data table\\nand narration word.\\n• Reinforcement learning. Moreover, some researchers adopt RL-based\\nsequence generation [34, 99, 101]. Shi et al. [99] build a reinforcement\\nlearning-based system to support the exploratory visual analysis of time\\nseries data. It constructs the agent’s state and action space with do-\\nmain knowledge to generate coherent data insights sequences as visual\\nanalysis recommendations. Specifically, the authors use the marko-\\nvian decision process (MDP) model to formulate an EVA sequence as\\na sequence of state-action pairs. Then, the RL-based method seeks to\\nmaximize the cumulative reward, which combines familiarity reward\\nand curiosity reward. In calculation of the curiosity reward, the casu-\\nalCNN model is used to embedding time sequences of different length\\ninto equal lengths.\\n6. Interaction\\nGenAI methods such as large language models have demonstrated great\\npotential for enhancing interaction in the broader field of human computer\\ninteraction [172, 9]. With GenAI methods, users can engage with the vi-\\nsualization charts, extracting novel insights and findings via a natural lan-\\nguage interface, as known as Chart Question Answering. Given a collection\\nof well-designed visualization charts, users can effortlessly navigate through\\nthis corpus to locate their desired chart using similarity search, which has\\nalso recently incorporated some GenAI techniques.\\n6.1. Visualization Retrieval\\nHaving established a set of well-crafted visualization charts and share on-\\nline, the subsequent question that arises is how we can help users in searching\\n38'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 38, 'page_label': '39'}, page_content='for their desired visualizations within a given repository effectively and ef-\\nficiently. This task is referred to as visualization retrieval. Engaging in\\nvisualization retrieval can offer significant advantages to several downstream\\ntasks such as learning visualization design [173, 174], visualization reuse [45],\\nvisualization corpus construction [175, 56], web mining [176, 177, 178], and\\ncomputational journalism [179].\\nRecently, some researchers adopt GenAI method to facilitate visualization\\nretrieval tailored to user intent about visual structure or other features.\\n6.1.1. Spatial Generation\\nData. Spatial generation methods for retrieval mainly involve raster visual-\\nization images.\\n• Visualization images . Recently, some studies leverage GenAI for en-\\nhanced representations of raster visualization images in retrieval, such\\nas WYTIWYR [57] and LineNet [56].\\nMethod. Methods for this task mainly include Triplet autoencoder and con-\\ntrastive learning.\\n• Triplet autoencoder. LineNet [56] addresses the problem of line chart\\nretrieval by considering both image-level and data-level similarity. For\\nthis purpose, a Triplet autoencoder is constructed with the backbone\\narchitecture of vision transformer [180]. Additionally, Luo et al. [56]\\nalso contribute a large-scale line chart corpus, named LineBench. This\\ncorpus contains over 115,000 line charts along with corresponding meta-\\ndata from four real-world datasets, facilitating the study of similarity\\nsearch in line chart visualizations.\\n• Contrastive learning. WYTIWYR [57] uses a contrastive language im-\\nage pretraining (CLIP) [181] to facilitate zero-shot user intent align-\\nment with visualization images.\\n6.1.2. Graph Generation\\nData. The data mainly involves SVG format visualizations.\\n• SVG. Graph representation is also used by a recent study [45] to incor-\\nporate structural information in SVG-format visualization retrieval.\\n39'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 39, 'page_label': '40'}, page_content='Method. The method mainly includes graph contrastive learning.\\n• Graph contrastive learning. Specifically, the InfoGraph [182] architec-\\nture is used, which is a contrastive graph representation learning model.\\nSpecifically, the model adopts GNN encoder and generate embedding\\nvector for input graph structure, where the optimization scheme takes\\nas input pairs of graphs and maximize the mutual information between\\none graph and its subgraph while minimizing mutual information with\\nthe subgraph in the other graph. Combining the graph representations\\nwith image-level visual representations, the visualization retrieval re-\\nsults prove to be more structurally consistent.\\n6.1.3. Discussion\\nRetrieval augmented generation (RAG). In the field of GenAI, a re-\\ncently popular topic is how to integrate retrieval into the generation pipeline\\nto achieve knowledge-grounded generation and reduce the uncertainty of\\npurely blackbox GenAI models [183, 184]. Recently some researchers also\\ncontemplate introducing such framework to visualization generation [80] to\\nreduce task complexity and increase reliability of generated results. This\\nwork focuses on generating DV query sequence. However, the RAG frame-\\nwork has the potential to benefit many more different GenAI applications in\\nvisualization. For example, for infographics generation, we can take the best\\nof both worlds by combining the previous retrieval-based methods [54, 161]\\nwith the latest purely GenAI methods [22]. In this way, users can bene-\\nfit from both the reliable real-world examples and the creativity of GenAI\\nmodels.\\nMulti-modal composed retrieval . WYTIWYR [57] introduces a re-\\ntrieval prototype with the novel composed query which combines image input\\nwith text describing users’ additional intent. Such multi-modal composed\\nretrieval has attracted considerable attention in the general domain of im-\\nage retrieval [185, 186, 187] and holds promise for improving the retrieval\\ninteraction for user intent alignment. Future work can further investigate\\nthe different combinations of multi-modal queries beyond text and image\\nmodalities and different logical compositions to allow for more flexible query\\ninteraction.\\n6.2. Chart Question Answering\\nIn data analysis with information visualization, sometimes only providing\\nthe charts to the users is not adequate as it might be time-consuming for them\\n40'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 40, 'page_label': '41'}, page_content='to comprehend complex information about the data in the chart. Chart\\nquestion answering (CQA) [188, 189, 190, 108, 191, 192] is a burgeoning\\nfield of research which seeks to develop intelligent algorithms and systems\\nto answer users’ questions about the charts to expedite data analysis and\\nenhance user interaction.\\n6.2.1. Sequence Generation\\nData. CQA mainly considers chart questions data.\\n• Chart questions. Chart questions can be categorized according to dif-\\nferent attributes [192], including factual/open-ended, visual/non-visual\\nand simple/compositional. In addition, in a more general sense, there\\ncan be different modalities of input as query about the chart.\\nMethod. The CQA methods mainly include chart elements detection and\\nvision-language model.\\n• Chart elements detection . Many AI-powered CQA models rely on de-\\ntection of chart elements and structure to facilitate extraction of rele-\\nvant information from visualization as explicit prior for generation of\\nanswers, such as PlotQA [102], FigureNet [103], DVQA [104], STL-\\nCQA [36] and LEAF-QA [105]. For example, PlotQA [102] utilizes\\nboth Visual Element Detection (VED) and Object Character Recogni-\\ntion (OCR) to extract key information from charts. In visual element\\ndetection, Faster R-CNN is used while in the OCR a traditional method\\nis adopted. Subsequently, the extracted chart elements is converted\\ninto a knowledge graph, which is combined with log-linear ranking of\\nlogical forms extracted from the question with compositional seman-\\ntic parsing to generate the answer. Other works adopt more complex\\nmodel fully based on neural network. For example, DVQA [104] de-\\nvelops a multi-output model that is capable of answering both generic\\nquestions and chart specific questions. The model contains an OCR\\nsub-network composed of a CNN-based bounding box predictor and a\\nGRU-based character-level decoder to extract the text. Based on the\\nresults of OCR sub-network, DVQA improves the Stacked Attention\\nNetwork (SAN) [193] for general visual question answering with addi-\\ntional dynamic encoding, which can adapt to chart specific vocabulary.\\nAlthough some most recent works still [106] rely on Chart elements\\n41'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 41, 'page_label': '42'}, page_content='Figure 9: Chart question answering examples in DVQA [104].\\ndetection and data extraction like ChartOCR [194] as an integral part,\\na few study [108] start to utilize a slightly different type of algorithm,\\nwhich is OCR-free document image understanding such as Donut [195].\\nDonut adopts the Swin Transformer architecture and is pretrained on\\nan OCR-pseudo task. However, in the inference stage it does not re-\\nquire external explicit OCR information and simply acts like an image\\nencoder.\\n• Vision-language model for multi-modal fusion. With the growing power\\nof generative AI, especially the multi-modal feature fusion capability,\\nsome recent studies simplifies the chart question answering pipeline\\nwith unified vision-language model, such as ChartQA [106], PReFIL [107],\\nUnichart [108] and ChartLlama [37]. For example, ChartQA [106]\\nbuilds a baseline model utilizing VL-T5 [196], a pre-trained unified\\nvision-language model for text generation conditioned on multi-modal\\ninputs. ChartQA also proposes their own model, VisionTaPas, which\\nis a multi-modal extension of the TaPas [197] model. The original\\nTaPas model is designed for answering questions over table, where a\\ntable is flattened into a sequence of words, converting the problem into\\nessentially a unimodal-input text generation task. For this task, the\\nBERT [198] architecture is extended with additional embeddings to rep-\\nresent table structure and context, including embeddings of segment,\\nrow/column, rank, and previous answer. In the VisionTaPas model, a\\nVision Transformer (ViT) [199] model is utilized to extract chart im-\\nage features into embeddings, as ViT has proven to be more powerful\\n42'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 42, 'page_label': '43'}, page_content='than CNN in many vision tasks. Next, a cross-modality encoder is\\nconstructed to fuse the multi-modal embeddings of ViT and TaPas,\\ncombining information of both text and chart images for end-to-end\\ngeneration of answers. Some recent research also explores instruction\\ntuning of pre-trained large vision-language model for more flexible gen-\\neration of answer [200].\\n6.2.2. Discussion\\nOther modalities . Most existing CQA systems only consider single\\nmodality natural language input as the primary means of interaction with\\nvisualization. However, other studies have shown the importance of interac-\\ntions in other modalities, such as body movement [201], touch and pen [202]\\nand gesture [203]. A few studies have attempted to achieve multi-modal\\ninputs by combining natural language or speech with mouse, pen or touch\\ninteractions [204, 205, 206]. Nevertheless, these tentative works rely on tra-\\nditional rule-based methods for quick prototyping and have not exploited the\\nlatest GenAI methods for CQA as introduced above. To achieve multi-modal\\nCQA, data-driven generative AI promises more flexibility than traditional\\nmethods in diverse real-world scenarios. For example, some researchers have\\nstarted to utilized GPT to support sketch interaction for generation of chart\\nfindings documentation [207].\\nCombination with data embedding . As shown in our introduction\\nof the algorithms, most GenAI-based CQA methods still depend on explicit\\ndetection of chart elements and underlying data for generation of precise\\nanswers. Such detection may not be always accurate and robust for complex\\nreal-world visualization images due to the diverse styles and visualization\\ntypes as well as additional noises. One possible strategy to circumvent this\\nissue is combining the data embedding method introduced in Section 3.2\\nwith CQA. With additional information about the data embedded in the\\nchart images, the performance of CQA can be expected to further improve.\\nThe promise of more precise vision-language model . Recent de-\\nvelopment of vision-language model is showing a trend of higher precision for\\nmore fine-grained detail in the images. Segment Anything [208] Model can\\nlocate specific semantic segment in the image given users visual or textual\\nprompt. Similarly, Grounding-Dino [209] can even more accurately generate\\nbounding box for particular objects in images with users’ prompts. In addi-\\ntion, LlaVa [210] allows users to flexibly ask about different levels of image\\ncontents from overall features to details. For the task of CQA which requires\\n43'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 43, 'page_label': '44'}, page_content='so much precision that additional detection models are needed, these pow-\\nerful vision-language models have the potential to significantly simplify the\\npipeline, leading towards a universal multi-modal model for chart interaction.\\nGenAI for visual analytics . Going beyond the interaction with sin-\\ngle charts, some researchers recently are exploring the possibility of extend-\\ning GenAI, particularly large language models to more complicated visual\\nanalytics workflow [109, 211]. For example, LEVA [109] utilizes LLM to\\nassist multiple stages of visual analytics including onboarding, exploration,\\nand summarization. With the development of LLM agent technology [212],\\nGenAI can potentially take the role of humans in some visual analytics tasks.\\nIt is an important question for future research to define new paradigm for\\nhuman-AI collaboration in data visualization and analysis.\\n7. Research Challenges And Opportunities\\n7.1. Evaluating GenAI for Visualization\\nThe increasing use of GenAI in the production of complex and creative\\nvisualizations. Given the essential role of rigorous evaluation in visualiza-\\ntion design, it becomes crucial to apply similar assessment standards to\\nAI-generated visualizations. The distinct characteristics and challenges pre-\\nsented by AI-driven visualization processes necessitate careful adaptation of\\nevaluation metrics and methodologies. While traditional metrics such as effi-\\nciency [21] and aesthetic [159] remain fundamental in evaluating AI-generated\\nvisualizations, the advent of AI techniques introduce additional, specific met-\\nrics that must be considered. From the migration of assessment metrics for\\nGenAI, the following assessment metrics are likely to be considered for eval-\\nuating differnet application of GenAI in visualization.\\n• Accuracy and Fidelity.Ensuring accuracy and fidelity in AI-generated\\nvisualizations is paramount, particularly when applying stylization tech-\\nniques. Techniques such as semantic contextualization in visualization\\n[22] face the challenge of balancing data integrity with aesthetic ap-\\npeal. This is crucial because real-life objects often do not conform to\\nthe rigid outlines typical in model-generated images, posing a risk to\\nthe accuracy of the visual representation.\\n• Intent Alignment and Controllbility. This criterion assesses the\\ndegree to which AI-generated visualizations align with the user’s intent\\n44'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 44, 'page_label': '45'}, page_content='Table 3: Examples of GenAI4VIS datasets.\\nDataset Data Format Source Supported Tasks\\nVizNet [217] Real world tables Web-crawled Data inference\\nVIS30K [125] Chart images Extracted from papers Data embedding\\nData2Vis [29] Table-code pairs Synthetic Table2VIS generation\\nnvBench [79] NL-code pairs Synthetic NL2VIS generation\\nMV [218] MV-layout labels Extracted from papers Layout transfer\\nChart-to-text [170] Chart-text pairs Web-crawled Text annotation\\nBeagle [219] SVG-type labels Web-crawled Visualization retrieval\\nLineBench [56] Chart-data pairs Synthesis with annotation Visualization retrieval\\nPlotQA [102] Chart-QA pairs Crowd-sourcing + Synthetic CQA\\nand their ability to influence the outcome. In Natural Language Inter-\\naction (NLI), controllability pertains to the user’s efficacy in steering\\nthe output of language-based AI systems during iterative interactions\\n[213, 214]. Furthermore, in end-to-end generation processes, it is es-\\nsential that the AI-generated visualizations are sensitive to and align\\nwith the user’s specific requirements, such as style or query objectives.\\n• Robustness and Consistency. Lastly, evaluating the robustness\\nand consistency of AI-generated visualizations across different scenar-\\nios is a key metric, ensuring reliability and applicability in diverse con-\\ntexts [215]. Regarding LLM, it may blending fact with fiction and gen-\\nerating non-factual content, which called hallucination problem [216].\\nFor example, in doing vqa tasks, especially in domains with specific re-\\nquirements for accuracy, evaluating the hallucinations of the generated\\ncontent is essential.\\n• Bias and Ethics. The potential biases inherent in AI algorithms and\\nthe ethical implications of their outputs necessitate careful examina-\\ntion.The generative model may face potential criticism on copyright\\nor bias issues, as the training process digests a huge amount of data\\nobtained from the web, which is unfiltered and imbalanced.\\nIn short, the field needs to update evaluation methods and criteria continually\\nto keep pace with advancing GenAI technologies in assessing AI-generated\\nvisualizations.\\n7.2. Dataset\\nAs GenAI is data-driven, these methods heavily depends on the training\\ndata. Indeed, most previous works applying GenAI to visualization build\\n45'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 45, 'page_label': '46'}, page_content='their own dataset or utilize the datasets created by prior works [23]. Even\\nin the era of large language models which are pre-trained on much larger\\ngeneral purpose dataset, a domain-specific visualization dataset can serve as\\nvaluable reference and knowledge base for efficient prompting and improving\\nthe reliability of GenAI results. The quality, quantity, and diversity of the\\ndataset thus have a significant impact on the generative performance and\\nthe output quality, as it determines how the GenAI model perceives and\\nunderstands the patterns and semantics of the generation requirement and\\ngenerated content.\\nIn this regard, several aspects warrant special attention in future research.\\nFirst, the diversity is important, as a diverse training dataset helps the AI\\nmodel learn a broader range of topics, styles and other design patterns in real-\\nworld visualization. This diversity enables the model to generate content that\\nis more versatile and contextually appropriate in different situations. How-\\never, many datasets used in training GenAI4VIS models are less diverse than\\nreal-world data [220], partly because most researchers collect or synthesize\\ntheir training data for prototyping of their generative methods, without suf-\\nficient consideration of more complex authentic cases. Therefore, building\\non existing GenAI4VIS studies, one important direction for future improve-\\nment is understanding the lack of diversity in current training datasets and\\nsupplement them accordingly.\\nSecond, the heterogeneous data in different formats can reduce the reusabil-\\nity. For instance, visualizations data are in a wide range of forms including\\nraster images, SVG and different types of codes like Vega-Lite and Python.\\nThis discrepancy necessitates the curation of datasets in different formats\\nanew for different generation tasks. This can also be a significant limiting\\nfactor for the size of the dataset because similar data in other formats cannot\\nbe utilized. This in turn may lead to overfitting and other difficulties in train-\\ning GenAI for visualization. To address this issue, more robust visualization\\nretargeting methods need to be developed to align different formats of data,\\nsuch as translating Vega-Lite code to Python code and extracting graphical\\nSVG structure from raster visualization images. For example, recently some\\nresearchers have been exploring the idea of using large language models to\\ngenerate various annotations for visualization datasets [97].\\nIn addition, we provide examples of some existing datasets that can be\\napplied to different GenAI4VIS tasks, as shown in Table 3. We can find that\\nsome researchers seek to address the lack of GenAI4VIS dataset by either col-\\nlecting and transforming real-world data or synthesizing data. For example,\\n46'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 46, 'page_label': '47'}, page_content='due to the lack of NL2VIS benchmarks, nvBench [79, 221] proposes utiliz-\\ning the existing NL2SQL benchmark dataset which can be transformed into\\nNL2VIS benchmark. Furthermore, we can see that some larger scale real-\\nworld datasets such as VIS30K [125] and VizNet [217] so far can only facili-\\ntate data enhancement tasks in GenAI4VIS because of the lack of annotations\\nabout the visual mapping process. As we mentioned above, how to strike a\\nbalance between synthetic methods with high scalability and real-world data\\ncollection which requires more manual effort or a complex retargeting process\\ncan be a critical issue. LineBench [56] is a large-scale line chart visualiza-\\ntion corpus (with 115,000 line charts) with the associated source dataset,\\nthe underlying data D for rendering, and the rendered visualization V in the\\nform of an image, which can facilitate the study of similarity search of line\\nchart visualizations. In this light, the perspective of data-centric explainable\\nAI [222, 223] is particularly relevant. Many visual analytics studies for ex-\\nplainable AI seek to help users explore the models from the data perspective\\nto gain insights about the potential biases, yet most of these works look at\\ngeneral-purpose AI or GenAI models. In other words, there is not enough\\nself-reflection studies from the visualization domain to diagnose GenAI4VIS\\nmodels along with their training data when most researchers are rushing to\\napply GenAI to various subtasks in the visualization pipeline.\\n7.3. GenAI4VIS vs. Generative Visualization\\nIn the area of digital art, there is some distinction between AI art and\\ngenerative art (or algorithmic art), where the latter term largely refers to\\ntraditional procedural generation algorithms with rule-based or optimization-\\nbased methods such as graph grammar or genetic algorithm [224]. In con-\\ntrast, AI art mostly encompasses end-to-end purely deep-learning-based gen-\\neration methods such as GAN, VAE or diffusion models. However, in the field\\nof visualization, there is little discussion about such distinction. In many\\nGenAI4VIS studies, researchers often introduce a hybrid approach, integrat-\\ning many rule-based constraints and procedures with partially AI-powered\\nmethods. For example, VizML [39] incorporates more than 800 hand-crafted\\nfeatures in the input, while restricting the generation output to predicting\\nonly a few basic visual structures. In essence, visualization has been relying\\non grammar-based generation which explicitly prescribe the mapping from\\ndata to visual structures and views with a suite of different codes and rules\\nsuch as Vega-Lite and visual design guidelines. To some extent, this can limit\\nthe effort to fully harness the power of GenAI, mainly because the models\\n47'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 47, 'page_label': '48'}, page_content='cannot directly learn the distribution of the final rendered images conditioned\\non data and user input, which is ultimately what visualization presents to\\nusers. This is vastly different from more mature GenAI applications in other\\nareas. For example, in spatial generation, latest GenAI technology can skip\\nmost traditional image synthesis and 3D modeling procedures and directly\\nrender the 2D images or 3D models. LIDA [32] makes an early effort towards\\na more integrated GenAI4VIS pipeline. However, LIDA’s pipeline is still di-\\nvided into separate sequence generation for visualization code and spatial\\ngeneration for visualization stylization in a linear workflow, where the two\\nGenAI models do not share knowledge. One problem due to this discon-\\nnection in LIDA, for example, is that the stylization stage cannot maintain\\nthe accuracy of the visual structure with respect to the data because the\\nimage-based stable diffusion model in the second stage is completely igno-\\nrant of structural information in the previous sequence generation. Moreover,\\nsome recent studies in AI show that merging two large pretrained models us-\\ning techniques like knowledge distillation can not only produce a versatile\\nmerged model but also boost the performance for downstream tasks that\\nrequire knowledge from both models [225], which provides inspiration for\\npotential strategies to improve integration of GenAI4VIS models.\\nIn fact, the gap between visualization and GenAI pipelines is not neces-\\nsarily a downside, as this signifies opportunities for future research to com-\\nbine the advantages while mitigating the respective disadvantages. On the\\none hand, visualization researchers can think about how to directly model\\nthe mapping between data and views in the end-to-end statistical learning\\nframework of GenAI, which can provide more effective learning and eval-\\nuation based on the final visual representations. For example, this means\\nthat raster visualization images from real world sources can also be directly\\nutilized as training data as long as it is annotated with user requirement\\ntext labels, without needing further complex explicit chart element extrac-\\ntion for retargeting. In this way, visualization can potentially harness the\\ntrue power of multi-modal GenAI based on large pretrained vision-language\\nmodels, which is showing more accurate control down to the pixel-level in\\nrecent research [226]. On the other hand, the intermediate operations like\\nthose in visualization should not be discarded by the GenAI4VIS pipeline be-\\ncause GenAI can be more explainable and controllable if users are allowed to\\ninspect and intervene in the key intermediate steps. However, such interven-\\ntion should not be the superficial rule-based constraints in hybrid methods.\\nInstead, researchers can take inspiration from GenAI research such as Con-\\n48'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 48, 'page_label': '49'}, page_content='trolNet [227] and LayoutDiffusion [228] to embed the control into the model\\nitself. Alternatively, researchers can develop interactive tools to support hu-\\nman intervention in the generation process [229].\\n8. Conclusion\\nThe burgeoning GenAI technology is promising for applications in the\\nvisualization domain. Because of GenAI’s impressive capacity to model the\\ntransformation and design process by learning from real data, it can ben-\\nefit a range of visualization tasks like data enhancement, visual mapping\\ngeneration, stylization and interaction. Different types of GenAI methods\\nhave been applied to these tasks due to different data structures, including\\nsequence generation, tabular generation, spatial generation and graph gener-\\nation. With the advent of latest GenAI technology like large language model\\nand diffusion model, new opportunities emerge to revolutionize GenAI4VIS\\nmethods. However, task-specific challenges still exist due to the unique\\ncharacteristics of visualization tasks, which demands further investigation.\\nMoreover, general challenges in evaluation and datasets require some rethink-\\ning about the GenAI4VIS pipeline beyond simply borrowing state-of-the art\\nGenAI methods. We hope this survey can help researchers reflect on existing\\nGenAI4VIS research from a technical perspective and provide some inspira-\\ntion for future research opportunities, with a vision for improved integration\\nof GenAI in visualization.\\nReferences\\n[1] A. Key, B. Howe, D. Perry, C. Aragon, VizDeck: self-organizing dash-\\nboards for visual analytics, in: Proceedings of the ACM SIGMOD\\nInternational Conference on Management of Data, 2012, pp. 681–684.\\n[2] S. Zhu, G. Sun, Q. Jiang, M. Zha, R. Liang, A survey on automatic in-\\nfographics and visualization recommendations, Visual Informatics 4 (3)\\n(2020) 24–40.\\n[3] A. Wu, Y. Wang, X. Shu, D. Moritz, W. Cui, H. Zhang, D. Zhang,\\nH. Qu, AI4VIS: Survey on artificial intelligence approaches for data vi-\\nsualization, IEEE Transactions on Visualization and Computer Graph-\\nics 28 (12) (2021) 5049–5070.\\n49'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 49, 'page_label': '50'}, page_content='[4] Q. Wang, Z. Chen, Y. Wang, H. Qu, A survey on ML4VIS: Applying\\nmachine learning advances to data visualization, IEEE Transactions on\\nVisualization and Computer Graphics 28 (12) (2021) 5134–5153.\\n[5] C. Wang, J. Han, Dl4SciVis: A state-of-the-art survey on deep learn-\\ning for scientific visualization, IEEE Transactions on Visualization and\\nComputer Graphics 29 (8) (2023) 3714–3733.\\n[6] X. Qin, Y. Luo, N. Tang, G. Li, Making data visualization more efficient\\nand effective: a survey, VLDB J. 29 (1) (2020) 93–117.\\n[7] M. Abukmeil, S. Ferrari, A. Genovese, V. Piuri, F. Scotti, A survey\\nof unsupervised generative models for exploratory data analysis and\\nrepresentation learning, ACM Computing Surveys 54 (5) (2021) 1–40.\\n[8] J. Gui, Z. Sun, Y. Wen, D. Tao, J. Ye, A review on generative adversar-\\nial networks: Algorithms, theory, and applications, IEEE Transactions\\non Knowledge and Data Engineering 35 (4) (2023) 3313–3332.\\n[9] Y. Hou, M. Yang, H. Cui, L. Wang, J. Xu, W. Zeng, C2Ideas: Support-\\ning creative interior color design ideation with large language model,\\narXiv preprint arXiv:2401.12586 (2024).\\n[10] S. Xiao, L. Wang, X. Ma, W. Zeng, TypeDance: Creating semantic\\ntypographic logos from image through personalized generation, arXiv\\npreprint arXiv:2401.11094 (2024).\\n[11] R. Huang, H.-C. Lin, C. Chen, K. Zhang, W. Zeng, PlantoGraphy:\\nIncorporating iterative design process into generative artificial intelli-\\ngence for landscape rendering, arXiv preprint arXiv:2401.17120 (2024).\\n[12] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, High-\\nresolution image synthesis with latent diffusion models, in: Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2022, pp. 10684–10695.\\n[13] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, M. Chen, Hierarchi-\\ncal text-conditional image generation with clip latents, arXiv preprint\\narXiv:2204.06125 (2022).\\n[14] OpenAI, GPT-4 technical report (2023). arXiv:2303.08774.\\n50'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 50, 'page_label': '51'}, page_content='[15] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi` ere, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and efficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023).\\n[16] B. Poole, A. Jain, J. T. Barron, B. Mildenhall, DreamFusion: Text-\\nto-3d using 2d diffusion, in: The International Conference on Learning\\nRepresentations, 2022.\\n[17] Y. Luo, X. Qin, N. Tang, G. Li, DeepEye: Towards automatic data\\nvisualization, in: Proceedings of the IEEE International Conference on\\nData Engineering, 2018, pp. 101–112.\\n[18] W. Cui, X. Zhang, Y. Wang, H. Huang, B. Chen, L. Fang, H. Zhang,\\nJ.-G. Lou, D. Zhang, Text-to-viz: Automatic generation of infographics\\nfrom proportion-related natural language statements, IEEE Transac-\\ntions on Visualization and Computer Graphics 26 (1) (2019) 906–916.\\n[19] J. Mackinlay, P. Hanrahan, C. Stolte, Show me: Automatic presenta-\\ntion for visual analysis, IEEE Transactions on Visualization and Com-\\nputer Graphics 13 (6) (2007) 1137–1144.\\n[20] M. Chen, D. Ebert, H. Hagen, R. S. Laramee, R. Van Liere, K.-L.\\nMa, W. Ribarsky, G. Scheuermann, D. Silver, Data, information, and\\nknowledge in visualization, IEEE Computer Graphics and Applications\\n29 (1) (2008) 12–19.\\n[21] E. R. Tufte, The visual display of quantitative information, Vol. 2,\\nGraphics press Cheshire, CT, 2001.\\n[22] S. Xiao, S. Huang, Y. Lin, Y. Ye, W. Zeng, Let the chart spark: Embed-\\nding semantic context into chart with text-to-image generative model,\\nIEEE Transactions on Visualization and Computer Graphics 30 (1)\\n(2024) 284–294.\\n[23] C. Chen, Z. Liu, The state of the art in creating visualization corpora\\nfor automated chart analysis, Computer Graphics Forum 42 (3) (2023)\\n449–470.\\n[24] C. R. Qi, H. Su, K. Mo, L. J. Guibas, PointNet: Deep learning on point\\nsets for 3d classification and segmentation, in: Proceedings of the IEEE\\n51'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 51, 'page_label': '52'}, page_content='conference on computer vision and pattern recognition, 2017, pp. 652–\\n660.\\n[25] S. K. Card, J. Mackinlay, B. Shneiderman, Readings in information\\nvisualization: using vision to think, Morgan Kaufmann, 1999.\\n[26] L. McNabb, R. S. Laramee, Survey of surveys (SoS)-mapping the land-\\nscape of survey papers in information visualization 36 (3) (2017) 589–\\n617.\\n[27] L. Shen, E. Shen, Y. Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, J. Wang,\\nTowards natural language interfaces for data visualization: A survey,\\nIEEE Transactions on Visualization and Computer Graphics 29 (6)\\n(2023) 3121–3144.\\n[28] M. Tennekes, E. de Jonge, Tree colors: color schemes for tree-structured\\ndata, IEEE Transactions on Visualization and Computer Graphics\\n20 (12) (2014) 2072–2081.\\n[29] V. Dibia, C ¸ . Demiralp, Data2vis: Automatic generation of data visu-\\nalizations using sequence-to-sequence recurrent neural networks, IEEE\\nComputer Graphics and Applications 39 (5) (2019) 33–46.\\n[30] M. Zhou, Q. Li, X. He, Y. Li, Y. Liu, W. Ji, S. Han, Y. Chen, D. Jiang,\\nD. Zhang, Table2Charts: Recommending charts by learning shared ta-\\nble representations, in: Proceedings of the ACM SIGKDD Conference\\non Knowledge Discovery & Data Mining, 2021, pp. 2389–2399.\\n[31] Y. Luo, N. Tang, G. Li, J. Tang, C. Chai, X. Qin, Natural language\\nto visualization by neural machine translation, IEEE Transactions on\\nVisualization and Computer Graphics (2021) 1–1.\\n[32] V. Dibia, LIDA: A tool for automatic generation of grammar-agnostic\\nvisualizations and infographics using large language models, in: Pro-\\nceedings of the Annual Meeting of the Association for Computational\\nLinguistics, 2023, pp. 113–126.\\n[33] A. Liew, K. Mueller, Using large language models to generate engag-\\ning captions for data visualizations, arXiv preprint arXiv:2212.14047\\n(2022).\\n52'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 52, 'page_label': '53'}, page_content='[34] D. Shi, X. Xu, F. Sun, Y. Shi, N. Cao, Calliope: Automatic visual data\\nstory generation from a spreadsheet, IEEE Transactions on Visualiza-\\ntion and Computer Graphics 27 (2) (2020) 453–463.\\n[35] C. Liu, L. Xie, Y. Han, D. Wei, X. Yuan, AutoCaption: An approach to\\ngenerate natural language description from visualization automatically,\\nin: IEEE Pacific Visualization Symposium (PacificVis), 2020, pp. 191–\\n195.\\n[36] H. Singh, S. Shekhar, STL-CQA: Structure-based transformers with lo-\\ncalization and encoding for chart question answering, in: Proceedings\\nof the Conference on Empirical Methods in Natural Language Process-\\ning, 2020, pp. 3275–3284.\\n[37] Y. Han, C. Zhang, X. Chen, X. Yang, Z. Wang, G. Yu, B. Fu, H. Zhang,\\nChartLlama: A multimodal llm for chart understanding and genera-\\ntion, arXiv preprint arXiv:2311.16483 (2023).\\n[38] N. Park, M. Mohammadi, K. Gorde, S. Jajodia, H. Park, Y. Kim, Data\\nsynthesis based on generative adversarial networks, Proceedings of the\\nVLDB Endowment 11 (10) (2018) 1071–1083.\\n[39] K. Hu, M. A. Bakker, S. Li, T. Kraska, C. Hidalgo, Vizml: A machine\\nlearning approach to visualization recommendation, in: Proceedings of\\nthe CHI Conference on Human Factors in Computing Systems, 2019,\\npp. 1–12.\\n[40] N. Shi, J. Xu, S. W. Wurster, H. Guo, J. Woodring, L. P. Van Roekel,\\nH.-W. Shen, GNN-Surrogate: A hierarchical and adaptive graph neural\\nnetwork for parameter space exploration of unstructured-mesh ocean\\nsimulations, IEEE Transactions on Visualization and Computer Graph-\\nics 28 (6) (2022) 2301–2313.\\n[41] Y. Zhang, J. Li, C. Xu, Graph-based latent space traversal for new\\nmolecules discovery, in: Proceedings of the International Symposium\\non Visual Information Communication and Interaction, 2023, pp. 1–8.\\n[42] H. Li, Y. Wang, S. Zhang, Y. Song, H. Qu, KG4Vis: A knowledge\\ngraph-based approach for visualization recommendation, IEEE Trans-\\nactions on Visualization and Computer Graphics 28 (1) (2021) 195–205.\\n53'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 53, 'page_label': '54'}, page_content='[43] O.-H. Kwon, K.-L. Ma, A deep generative model for graph layout,\\nIEEE Transactions on Visualization and Computer Graphics 26 (1)\\n(2019) 665–675.\\n[44] S. Song, C. Li, Y. Sun, C. Wang, VividGraph: Learning to extract and\\nredesign network graphs from visualization images, IEEE Transactions\\non Visualization and Computer Graphics 29 (7) (2023) 3169–3181.\\n[45] H. Li, Y. Wang, A. Wu, H. Wei, H. Qu, Structure-aware visualization\\nretrieval, in: Proceedings of the CHI Conference on Human Factors in\\nComputing Systems, 2022, pp. 1–14.\\n[46] Y. Liu, E. Jun, Q. Li, J. Heer, Latent space cartography: Visual anal-\\nysis of vector space embeddings, Computer Graphics Forum 38 (3)\\n(2019) 67–78.\\n[47] J. Han, H. Zheng, D. Z. Chen, C. Wang, STNet: An end-to-end gener-\\native framework for synthesizing spatiotemporal super-resolution vol-\\numes, IEEE Transactions on Visualization and Computer Graphics\\n28 (1) (2021) 270–280.\\n[48] L. Gou, L. Zou, N. Li, M. Hofmann, A. K. Shekar, A. Wendt, L. Ren,\\nVATLD: A visual analytics system to assess, understand and improve\\ntraffic light detection, IEEE Transactions on Visualization and Com-\\nputer Graphics 27 (2) (2020) 261–271.\\n[49] P. Zhang, C. Li, C. Wang, VisCode: Embedding information in visual-\\nization images using encoder-decoder network, IEEE Transactions on\\nVisualization and Computer Graphics 27 (2) (2020) 326–336.\\n[50] Y. L. Huayuan Ye, Chenhui Li, C. Wang, InvVis: Large-scale data\\nembedding for invertible visualization, IEEE Transactions on Visual-\\nization and Computer Graphics 30 (1) (2024) 1139–1149.\\n[51] R. Ma, H. Mei, H. Guan, W. Huang, F. Zhang, C. Xin, W. Dai, X. Wen,\\nW. Chen, LADV: Deep learning assisted authoring of dashboard visual-\\nizations from images and sketches, IEEE Transactions on Visualization\\nand Computer Graphics 27 (9) (2020) 3717–3732.\\n[52] C. Chen, C. Wang, X. Bai, P. Zhang, C. Li, Generativemap: Visualiza-\\ntion and exploration of dynamic density maps via generative learning\\n54'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 54, 'page_label': '55'}, page_content='model, IEEE Transactions on Visualization and Computer Graphics\\n26 (1) (2019) 216–226.\\n[53] L.-P. Yuan, W. Zeng, S. Fu, Z. Zeng, H. Li, C.-W. Fu, H. Qu, Deep\\ncolormap extraction from visualizations, IEEE Transactions on Visu-\\nalization and Computer Graphics 28 (12) (2021) 4048–4060.\\n[54] Y. Shi, P. Liu, S. Chen, M. Sun, N. Cao, Supporting expressive and\\nfaithful pictorial visualization design with visual style transfer, IEEE\\nTransactions on Visualization and Computer Graphics 29 (1) (2022)\\n236–246.\\n[55] T. Tang, R. Li, X. Wu, S. Liu, J. Knittel, S. Koch, T. Ertl, L. Yu,\\nP. Ren, Y. Wu, Plotthread: Creating expressive storyline visualizations\\nusing reinforcement learning, IEEE Transactions on Visualization and\\nComputer Graphics 27 (2) (2020) 294–303.\\n[56] Y. Luo, Y. Zhou, N. Tang, G. Li, C. Chai, L. Shen, Learned data-aware\\nimage representations of line charts for similarity search, Proceedings\\nof the ACM on Management of Data 1 (1) (2023) 88:1–88:29.\\n[57] S. Xiao, Y. Hou, C. Jin, W. Zeng, WYTIWYR: A user intent-aware\\nframework with multi-modal inputs for visualization retrieval, Com-\\nputer Graphics Forum 42 (3) (2023) 311–322.\\n[58] J. Xia, J. Li, S. Chen, H. Qin, S. Liu, A survey on interdisciplinary\\nresearch of visualization and artificial intelligence, Scientia Sinica (In-\\nformationis) 51 (2021) 1777–1801.\\n[59] Q. Chen, S. Cao, J. Wang, N. Cao, How does automation shape the\\nprocess of narrative visualization: A survey of tools, IEEE Transactions\\non Visualization and Computer Graphics (2023) 1–20.\\n[60] Y. He, S. Cao, Y. Shi, Q. Chen, K. Xu, N. Cao, Leveraging large\\nmodels for crafting narrative visualization: A survey, arXiv preprint\\narXiv:2401.14010 (2024).\\n[61] S. Di Bartolomeo, V. Schetinger, J. L. Adams, A. M. McNutt, M. El-\\nAssady, M. Miller, Doom or deliciousness: Challenges and opportuni-\\nties for visualization in the age of generative models, Computer Graph-\\nics Forum 42 (3) (2023) 423–435.\\n55'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 55, 'page_label': '56'}, page_content='[62] W. Yang, M. Liu, Z. Wang, S. Liu, Foundation models meet visual-\\nizations: Challenges and opportunities, Computational Visual Media\\n(2023).\\n[63] J. Fan, T. Liu, G. Li, J. Chen, Y. Shen, X. Du, Relational data synthe-\\nsis using generative adversarial networks: A design space exploration,\\nProceedings of the VLDB Endowment 13 (11) (2020) 1962–1975.\\n[64] H. Chen, S. Jajodia, J. Liu, N. Park, V. Sokolov, V. Subrahmanian,\\nFakeTables: Using gans to generate functional dependency preserving\\ntables with bounded real data., in: Proceedings of IJCAI, 2019, pp.\\n2074–2080.\\n[65] X. Qinl, C. Chai, N. Tang, J. Li, Y. Luo, G. Li, Y. Zhu, Synthesizing\\nprivacy preserving entity resolution datasets, in: Proceedings of IEEE\\nInternational Conference on Data Engineering, 2022, pp. 2359–2371.\\n[66] L. Xu, M. Skoularidou, A. Cuesta-Infante, K. Veeramachaneni, Model-\\ning tabular data using conditional gan, in: Proceedings of the Interna-\\ntional Conference on Neural Information Processing Systems, Vol. 32,\\n2019.\\n[67] J. Zhang, G. Cormode, C. M. Procopiuc, D. Srivastava, X. Xiao,\\nPrivBayes: Private data release via bayesian networks, ACM Trans-\\nactions on Database Systems (TODS) 42 (4) (2017) 1–41.\\n[68] Y. Wang, Z. Zhong, J. Hua, DeepOrganNet: on-the-fly reconstruction\\nand visualization of 3d/4d lung models from single-view projections\\nby deep deformation network, IEEE Transactions on Visualization and\\nComputer Graphics 26 (1) (2019) 960–970.\\n[69] W. He, L. Zou, A. K. Shekar, L. Gou, L. Ren, Where can we help? a\\nvisual analytics approach to diagnosing and improving semantic seg-\\nmentation of movable objects, IEEE Transactions on Visualization and\\nComputer Graphics 28 (1) (2021) 1040–1050.\\n[70] Z. Zhou, Y. Hou, Q. Wang, G. Chen, J. Lu, Y. Tao, H. Lin, Volume\\nupscaling with convolutional neural networks, in: Proceedings of the\\nComputer Graphics International Conference, 2017, pp. 1–6.\\n56'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 56, 'page_label': '57'}, page_content='[71] S. Wiewel, M. Becher, N. Thuerey, Latent space physics: Towards\\nlearning the temporal evolution of fluid flow, Computer Graphics Fo-\\nrum 38 (2) (2019) 71–82.\\n[72] Q. Wang, S. L’Yi, N. Gehlenborg, DRAVA: Aligning human concepts\\nwith machine learning latent dimensions for the visual exploration of\\nsmall multiples, in: Proceedings of the CHI Conference on Human\\nFactors in Computing Systems, 2023, pp. 1–15.\\n[73] J. Wang, W. Zhang, H. Yang, SCANViz: Interpreting the symbol-\\nconcept association captured by deep neural networks through vi-\\nsual analytics, in: IEEE Pacific Visualization Symposium (PacificVis),\\n2020, pp. 51–60.\\n[74] N. Evirgen, X. Chen, GANravel: User-driven direction disentangle-\\nment in generative adversarial networks, in: Proceedings of the CHI\\nConference on Human Factors in Computing Systems, 2023, pp. 1–15.\\n[75] H. Singh, N. McCarthy, Q. U. Ain, J. Hayes, ChemoVerse: Manifold\\ntraversal of latent spaces for novel molecule discovery, arXiv preprint\\narXiv:2009.13946 (2020).\\n[76] W. Zheng, J. Li, Y. Zhang, Desirable molecule discovery via generative\\nlatent space exploration, Visual Informatics 7 (4) (2023) 13–21.\\n[77] J. Fu, B. Zhu, W. Cui, S. Ge, Y. Wang, H. Zhang, H. Huang, Y. Tang,\\nD. Zhang, X. Ma, Chartem: reviving chart images with data em-\\nbedding, IEEE Transactions on Visualization and Computer Graphics\\n27 (2) (2020) 337–346.\\n[78] C. Liu, Y. Han, R. Jiang, X. Yuan, Advisor: Automatic visualization\\nanswer for natural-language question on tabular data, in: IEEE Pacific\\nVisualization Symposium (PacificVis), IEEE, 2021, pp. 11–20.\\n[79] Y. Luo, N. Tang, G. Li, C. Chai, W. Li, X. Qin, Synthesizing Nat-\\nural Language to Visualization (NL2VIS) Benchmarks from NL2SQL\\nBenchmarks, in: Proceedings of the International Conference on Man-\\nagement of Data, p. 1235–1247.\\n57'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 57, 'page_label': '58'}, page_content='[80] Y. Song, X. Zhao, R. C.-W. Wong, D. Jiang, RGVisNet: A hybrid\\nretrieval-generation neural framework towards automatic data visual-\\nization generation, in: Proceedings of the ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, 2022, pp. 1646–1655.\\n[81] L. Wang, S. Zhang, Y. Wang, E.-P. Lim, Y. Wang, LLM4Vis: Explain-\\nable visualization recommendation using ChatGPT, arXiv preprint\\narXiv:2310.07652 (2023).\\n[82] C. Shi, W. Cui, C. Liu, C. Zheng, H. Zhang, Q. Luo, X. Ma, NL2Color:\\nRefining color palettes for charts with natural language, IEEE Transac-\\ntions on Visualization and Computer Graphics 30 (1) (2024) 814–824.\\n[83] S. Li, X. Chen, Y. Song, Y. Song, C. Zhang, Prompt4Vis: Prompting\\nlarge language models with example mining and schema filtering for\\ntabular data visualization, arXiv preprint arXiv:2402.07909 (2024).\\n[84] Y. Tian, W. Cui, D. Deng, X. Yi, Y. Yang, H. Zhang, Y. Wu, Chart-\\nGPT: Leveraging llms to generate charts from abstract natural lan-\\nguage, IEEE Transactions on Visualization and Computer Graphics\\n(2024) 1–15.\\n[85] G. Li, X. Wang, G. Aodeng, S. Zheng, Y. Zhang, C. Ou, S. Wang,\\nC. H. Liu, Visualization generation with large language models: An\\nevaluation, arXiv preprint arXiv:2401.11255 (2024).\\n[86] Y. Luo, X. Qin, N. Tang, G. Li, X. Wang, Deepeye: Creating good data\\nvisualizations by keyword search, in: Proceedings of the International\\nConference on Management of Data, 2018, pp. 1733–1736.\\n[87] M. Berger, J. Li, J. A. Levine, A generative model for volume rendering,\\nIEEE Transactions on Visualization and Computer Graphics 25 (4)\\n(2018) 1636–1650.\\n[88] F. Hong, C. Liu, X. Yuan, DNN-VolVis: Interactive volume visualiza-\\ntion supported by deep neural network, in: IEEE Pacific Visualization\\nSymposium (PacificVis), 2019, pp. 282–291.\\n[89] L. Wu, J. Y. Lee, A. Bhattad, Y.-X. Wang, D. Forsyth, DIVeR: Real-\\ntime and accurate neural radiance fields with deterministic integration\\n58'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 58, 'page_label': '59'}, page_content='for volume rendering, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16200–16209.\\n[90] W. Gan, H. Xu, Y. Huang, S. Chen, N. Yokoya, V4d: Voxel for 4d\\nnovel view synthesis, IEEE Transactions on Visualization and Com-\\nputer Graphics (2023) 1–14.\\n[91] D. Huang, J. Wang, G. Wang, C.-Y. Lin, Visual style extraction from\\nchart images for chart restyling, in: Proceedings of the International\\nConference on Pattern Recognition, 2021, pp. 7625–7632.\\n[92] S. Liu, M. Tao, Y. Huang, C. Wang, C. Li, Image-driven harmonious\\ncolor palette generation for diverse information visualization, IEEE\\nTransactions on Visualization and Computer Graphics (2022) 1–16.\\n[93] Z. Chen, Y. Wang, Q. Wang, Y. Wang, H. Qu, Towards automated\\ninfographic design: Deep learning-based auto-extraction of extensible\\ntimeline, IEEE Transactions on Visualization and Computer Graphics\\n26 (1) (2019) 917–926.\\n[94] M. Lu, C. Wang, J. Lanir, N. Zhao, H. Pfister, D. Cohen-Or, H. Huang,\\nExploring visual information flows in infographics, in: Proceedings of\\nthe CHI conference on human factors in computing systems, 2020, pp.\\n1–12.\\n[95] Y. Wang, Z. Jin, Q. Wang, W. Cui, T. Ma, H. Qu, DeepDrawing:\\nA deep learning approach to graph drawing, IEEE Transactions on\\nVisualization and Computer Graphics 26 (1) (2019) 676–686.\\n[96] J. Wu, J. J. Y. Chung, E. Adar, viz2viz: Prompt-driven styl-\\nized visualization generation using a diffusion model, arXiv preprint\\narXiv:2304.01919 (2023).\\n[97] H.-K. Ko, H. Jeon, G. Park, D. H. Kim, N. W. Kim, J. Kim, J. Seo,\\nNatural language dataset generation framework for visualizations pow-\\nered by large language models, arXiv preprint arXiv:2309.10245 (2023).\\n[98] L. Shen, Y. Zhang, H. Zhang, Y. Wang, Data player: Automatic gener-\\nation of data videos with narration-animation interplay, IEEE Transac-\\ntions on Visualization and Computer Graphics 30 (1) (2024) 109–119.\\n59'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 59, 'page_label': '60'}, page_content='[99] Y. Shi, B. Chen, Y. Chen, Z. Jin, K. Xu, X. Jiao, T. Gao, N. Cao,\\nSupporting Guided Exploratory Visual Analysis on Time Series Data\\nwith Reinforcement Learning, IEEE Transactions on Visualization and\\nComputer Graphics (2023) 1–11.\\n[100] L. Ying, Y. Wang, H. Li, S. Dou, H. Zhang, X. Jiang, H. Qu, Y. Wu,\\nReviving static charts into live charts, arXiv preprint arXiv:2309.02967\\n(2023).\\n[101] G. Wu, S. Guo, J. Hoffswell, G. Y.-Y. Chan, R. A. Rossi, E. Koh,\\nSocrates: Data story generation via adaptive machine-guided elicita-\\ntion of user feedback, IEEE Transactions on Visualization and Com-\\nputer Graphics 30 (1) (2024) 131–141.\\n[102] N. Methani, P. Ganguly, M. M. Khapra, P. Kumar, PlotQA: Reasoning\\nover scientific plots, in: Proceedings of the IEEE/CVF Winter Confer-\\nence on Applications of Computer Vision, 2020, pp. 1527–1536.\\n[103] R. Reddy, R. Ramesh, A. Deshpande, M. M. Khapra, FigureNet: A\\ndeep learning model for question-answering on scientific plots, in: In-\\nternational Joint Conference on Neural Networks, IEEE, 2019, pp. 1–8.\\n[104] K. Kafle, B. Price, S. Cohen, C. Kanan, DVQA: Understanding data\\nvisualizations via question answering, in: Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, 2018, pp.\\n5648–5656.\\n[105] R. Chaudhry, S. Shekhar, U. Gupta, P. Maneriker, P. Bansal, A. Joshi,\\nLeaf-QA: Locate, encode & attend for figure question answering, in:\\nProceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision, 2020, pp. 3512–3521.\\n[106] A. Masry, X. L. Do, J. Q. Tan, S. Joty, E. Hoque, ChartQA: A bench-\\nmark for question answering about charts with visual and logical rea-\\nsoning, in: Findings of the Association for Computational Linguistics,\\n2022, pp. 2263–2279.\\n[107] K. Kafle, R. Shrestha, S. Cohen, B. Price, C. Kanan, Answering ques-\\ntions about data visualizations using efficient bimodal fusion, in: Pro-\\nceedings of the IEEE/CVF Winter Conference on Applications of Com-\\nputer Vision, 2020, pp. 1498–1507.\\n60'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 60, 'page_label': '61'}, page_content='[108] A. Masry, P. Kavehzadeh, X. L. Do, E. Hoque, S. Joty, UniChart:\\nA universal vision-language pretrained model for chart comprehension\\nand reasoning, in: Proceedings of the Conference on Empirical Methods\\nin Natural Language Processing, 2023, pp. 14662–14684.\\n[109] Y. Zhao, Y. Zhang, Y. Zhang, X. Zhao, J. Wang, Z. Shao, C. Turkay,\\nS. Chen, LEVA: Using large language models to enhance visual an-\\nalytics, IEEE Transactions on Visualization and Computer Graphics\\n(2024) 1–17.\\n[110] R. G´ omez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern´ andez-\\nLobato, B. S´ anchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre,\\nT. D. Hirzel, R. P. Adams, A. Aspuru-Guzik, Automatic chemical de-\\nsign using a data-driven continuous representation of molecules, ACS\\nCentral Science 4 (2) (2018) 268–276.\\n[111] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, A compre-\\nhensive survey on graph neural networks, IEEE Transactions on Neural\\nNetworks and Learning Systems 32 (1) (2020) 4–24.\\n[112] T. White, Sampling generative networks, arXiv preprint\\narXiv:1609.04468 (2016).\\n[113] J. Y. Yen, Finding the k shortest loopless paths in a network, Manage-\\nment Science 17 (11) (1971) 712–716.\\n[114] A. Radford, L. Metz, S. Chintala, Unsupervised representation learning\\nwith deep convolutional generative adversarial networks, arXiv preprint\\narXiv:1511.06434 (2015).\\n[115] Y. Li, B. Sixou, F. Peyrin, A review of the deep learning methods for\\nmedical images super resolution problems, Innovation and Research in\\nBioMedical Engineering 42 (2) (2021) 120–133.\\n[116] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, S.-H. Lai,\\nAugGan: Cross domain adaptation with gan-based data augmentation,\\nin: Proceedings of the European Conference on Computer Vision, 2018,\\npp. 718–731.\\n61'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 61, 'page_label': '62'}, page_content='[117] J. Choi, T. Kim, C. Kim, Self-ensembling with gan-based data augmen-\\ntation for domain adaptation in semantic segmentation, in: Proceed-\\nings of the IEEE/CVF International Conference on Computer Vision,\\n2019, pp. 6830–6840.\\n[118] X. Liu, Y. Zou, L. Kong, Z. Diao, J. Yan, J. Wang, S. Li, P. Jia,\\nJ. You, Data augmentation via latent space interpolation for image\\nclassification, in: Proceedings of the International Conference on Pat-\\ntern Recognition (ICPR), 2018, pp. 728–733.\\n[119] D. P. Kingma, M. Welling, Auto-encoding variational bayes, arXiv\\npreprint arXiv:1312.6114 (2013).\\n[120] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial networks,\\nCommunications of the ACM 63 (11) (2020) 139–144.\\n[121] L. Tran, X. Yin, X. Liu, Disentangled representation learning gan for\\npose-invariant face recognition, in: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2017, pp. 1415–1424.\\n[122] I. Jeon, W. Lee, M. Pyeon, G. Kim, Ib-GAN: Disentangled represen-\\ntation learning with information bottleneck generative adversarial net-\\nworks, in: Proceedings of the AAAI Conference on Artificial Intelli-\\ngence, Vol. 35, 2021, pp. 7926–7934.\\n[123] C. Zhou, F. Zhong, C. ¨Oztireli, CLIP-PAE: Projection-augmentation\\nembedding to extract relevant features for a disentangled, interpretable\\nand controllable text-guided face manipulation, in: Proceedings of the\\nACM SIGGRAPH Conference, 2023, pp. 1–9.\\n[124] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Des-\\njardins, A. Lerchner, Understanding disentangling in beta-vae, arXiv\\npreprint arXiv:1804.03599 (2018).\\n[125] J. Chen, M. Ling, R. Li, P. Isenberg, T. Isenberg, M. Sedlmair,\\nT. M¨ oller, R. S. Laramee, H.-W. Shen, K. W¨ unsche, et al., Vis30k: A\\ncollection of figures and tables from ieee visualization conference pub-\\nlications, IEEE Transactions on Visualization and Computer Graphics\\n27 (9) (2021) 3826–3833.\\n62'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 62, 'page_label': '63'}, page_content='[126] M. A. Borkin, A. A. Vo, Z. Bylinskii, P. Isola, S. Sunkavalli, A. Oliva,\\nH. Pfister, What makes a visualization memorable?, IEEE Transactions\\non Visualization and Computer Graphics 19 (12) (2013) 2306–2315.\\n[127] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, M. Jagersand, BAS-\\nNet: Boundary-aware salient object detection, in: Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2019, pp. 7479–7489.\\n[128] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely con-\\nnected convolutional networks, in: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2017, pp. 4700–4708.\\n[129] L. Dinh, D. Krueger, Y. Bengio, Nice: Non-linear independent compo-\\nnents estimation, arXiv preprint arXiv:1410.8516 (2014).\\n[130] A. Narechania, A. Srinivasan, J. Stasko, NL4DV: A toolkit for generat-\\ning analytic specifications for data visualization from natural language\\nqueries, IEEE Transactions on Visualization and Computer Graphics\\n27 (2) (2020) 369–379.\\n[131] J. Gu, Z. Lu, H. Li, V. O. Li, Incorporating copying mechanism in\\nsequence-to-sequence learning, in: Proceedings of the Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long\\nPapers), 2016, pp. 1631–1640.\\n[132] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, I. Polosukhin, Attention is all you need, in: Proceed-\\nings of the International Conference on Neural Information Processing\\nSystems, 2017.\\n[133] M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek,\\nM. Chen, Efficient training of language models to fill in the middle,\\narXiv preprint arXiv:2207.14255 (2022).\\n[134] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022).\\n[135] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez,\\nN. Schiefer, Z. Hatfield-Dodds, N. DasSarma, E. Tran-Johnson, et al.,\\n63'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 63, 'page_label': '64'}, page_content='Language models (mostly) know what they know, arXiv preprint\\narXiv:2207.05221 (2022).\\n[136] S. Hegselmann, A. Buendia, H. Lang, M. Agrawal, X. Jiang, D. Son-\\ntag, TabLLM: Few-shot classification of tabular data with large lan-\\nguage models, in: International Conference on Artificial Intelligence\\nand Statistics, PMLR, 2023, pp. 5549–5581.\\n[137] Y. Luo, X. Qin, C. Chai, N. Tang, G. Li, W. Li, Steerable self-driving\\ndata visualization, IEEE Transactions on Knowledge and Data Engi-\\nneering 34 (1) (2020) 475–490.\\n[138] X. Qin, Y. Luo, N. Tang, G. Li, DeepEye: An automatic big data\\nvisualization framework, Big Data Mining and Analytics 1 (1) (2018)\\n75–82.\\n[139] X. Qin, Y. Luo, N. Tang, G. Li, Deepeye: Visualizing your data by\\nkeyword search, in: EDBT, OpenProceedings.org, 2018, pp. 441–444.\\n[140] A. Voynov, K. Aberman, D. Cohen-Or, Sketch-guided text-to-image\\ndiffusion models, in: ACM SIGGRAPH Conference Proceedings, 2023,\\npp. 1–11.\\n[141] Z. Teng, Q. Fu, J. White, D. C. Schmidt, Sketch2Vis: Generating\\ndata visualizations from hand-drawn sketches with deep learning, in:\\nProceedings of the IEEE International Conference on Machine Learning\\nand Applications (ICMLA), 2021, pp. 853–858.\\n[142] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time\\nobject detection with region proposal networks, in: Proceedings of the\\nInternational Conference on Neural Information Processing Systems,\\nVol. 28, 2015.\\n[143] J. Donahue, P. Kr¨ ahenb¨ uhl, T. Darrell, Adversarial feature learning,\\narXiv preprint arXiv:1605.09782 (2016).\\n[144] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image\\nrecognition, in: Proceedings of the Conference on Computer Vision\\nand Pattern Recognition, 2016, pp. 770–778.\\n64'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 64, 'page_label': '65'}, page_content='[145] Z. Sun, Z.-H. Deng, J.-Y. Nie, J. Tang, RotatE: Knowledge graph\\nembedding by relational rotation in complex space, in: International\\nConference on Learning Representations, 2018.\\n[146] K. Han, Y. Wang, J. Guo, Y. Tang, E. Wu, Vision GNN: An image is\\nworth graph of nodes, in: Proceedings of the International Conference\\non Neural Information Processing Systems, Vol. 35, 2022, pp. 8291–\\n8303.\\n[147] E. Sella, G. Fiebelman, P. Hedman, H. Averbuch-Elor, Vox-E: Text-\\nguided voxel editing of 3d objects, in: Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, 2023, pp. 430–440.\\n[148] J. Harper, M. Agrawala, Converting basic d3 charts into reusable style\\ntemplates, IEEE Transactions on Visualization and Computer Graph-\\nics 24 (3) (2017) 1274–1286.\\n[149] M. Bostock, V. Ogievetsky, J. Heer, D 3 data-driven documents, IEEE\\nTransactions on Visualization and Computer Graphics 17 (12) (2011)\\n2301–2309.\\n[150] Z. Bylinskii, N. W. Kim, P. O’Donovan, S. Alsheikh, S. Madan, H. Pfis-\\nter, F. Durand, B. Russell, A. Hertzmann, Learning visual importance\\nfor graphic designs and data visualizations, in: Proceedings of the An-\\nnual ACM symposium on user interface software and technology, 2017,\\npp. 57–69.\\n[151] L.-P. Yuan, Z. Zhou, J. Zhao, Y. Guo, F. Du, H. Qu, InfoColorizer:\\nInteractive recommendation of color palettes for infographics, IEEE\\nTransactions on Visualization and Computer Graphics 28 (12) (2021)\\n4252–4266.\\n[152] Y. Shi, S. Chen, P. Liu, J. Long, N. Cao, Colorcook: Augmenting color\\ndesign for dashboarding with domain-associated palettes, Proceedings\\nof the ACM on Human-Computer Interaction 6 (CSCW2) (2022) 1–25.\\n[153] M. Lagunas, E. Garces, D. Gutierrez, Learning icons appearance simi-\\nlarity, Multimedia Tools and Applications 78 (2019) 10733–10751.\\n65'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 65, 'page_label': '66'}, page_content='[154] A. Grover, J. Leskovec, node2vec: Scalable feature learning for net-\\nworks, in: Proceedings of the ACM SIGKDD International Conference\\non Knowledge Discovery and Data Mining, 2016, pp. 855–864.\\n[155] Q. Zheng, M. Lu, S. Wu, R. Hu, J. Lanir, H. Huang, Image-guided\\ncolor mapping for categorical data visualization, Computational Visual\\nMedia 8 (4) (2022) 613–629.\\n[156] M. Wu, Y. Sun, S. Jiang, Adaptive color transfer from images to ter-\\nrain visualizations, IEEE Transactions on Visualization and Computer\\nGraphics (2023) 1–16.\\n[157] J. Poco, A. Mayhua, J. Heer, Extracting and retargeting color map-\\npings from bitmap images of visualizations, IEEE Transactions on Vi-\\nsualization and Computer Graphics 24 (1) (2017) 637–646.\\n[158] R. Borgo, A. Abdul-Rahman, F. Mohamed, P. W. Grant, I. Reppa,\\nL. Floridi, M. Chen, An empirical study on using visual embellishments\\nin visualization, IEEE Transactions on Visualization and Computer\\nGraphics 18 (12) (2012) 2759–2768.\\n[159] L. Harrison, K. Reinecke, R. Chang, Infographic aesthetics: Designing\\nfor the first impression, in: Proceedings of the ACM Conference on\\nHuman Factors in Computing Systems, 2015, pp. 1187–1190.\\n[160] S. Haroz, R. Kosara, S. L. Franconeri, Isotype visualization: Working\\nmemory, performance, and engagement with pictographs, in: Proceed-\\nings of ACM Conference on Human Factors in Computing Systems,\\n2015, pp. 1191–1200.\\n[161] D. Coelho, K. Mueller, Infomages: Embedding data into thematic im-\\nages, Computer Graphics Forum 39 (3) (2020) 593–606.\\n[162] J. E. Zhang, N. Sultanum, A. Bezerianos, F. Chevalier, Dataquilt: Ex-\\ntracting visual elements from images to craft pictorial visualizations, in:\\nProceedings of the CHI Conference on Human Factors in Computing\\nSystems, 2020, pp. 1–13.\\n[163] J. Cheng, X. Liang, X. Shi, T. He, T. Xiao, M. Li, Layoutdiffuse:\\nAdapting foundational diffusion models for layout-to-image generation,\\narXiv preprint arXiv:2302.08908 (2023).\\n66'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 66, 'page_label': '67'}, page_content='[164] J. Chen, Y. Huang, T. Lv, L. Cui, Q. Chen, F. Wei, Textdiffuser: Dif-\\nfusion models as text painters, arXiv preprint arXiv:2305.10855 (2023).\\n[165] P. Zhang, N. Zhao, J. Liao, Text-guided vector graphics customization,\\narXiv preprint arXiv:2309.12302 (2023).\\n[166] E. Segel, J. Heer, Narrative visualization: Telling stories with data,\\nIEEE Transactions on Visualization and Computer Graphics 16 (6)\\n(2010) 1139–1148.\\n[167] A. Cairo, The Functional Art: An introduction to information graphics\\nand visualization, New Riders, 2012.\\n[168] J. Hullman, N. Diakopoulos, Visualization rhetoric: Framing effects in\\nnarrative visualization, IEEE Transactions on Visualization and Com-\\nputer Graphics 17 (12) (2011) 2231–2240.\\n[169] C. Lai, Z. Lin, R. Jiang, Y. Han, C. Liu, X. Yuan, Automatic an-\\nnotation synchronizing with textual description for visualization, in:\\nProceedings of the 2020 CHI Conference on Human Factors in Com-\\nputing Systems, 2020, pp. 1–13.\\n[170] S. Kantharaj, R. T. Leong, X. Lin, A. Masry, M. Thakkar, E. Hoque,\\nS. Joty, Chart-to-Text: A large-scale benchmark for chart summariza-\\ntion, in: Proceedings of the Annual Meeting of the Association for\\nComputational Linguistics, 2022, pp. 4005–4023.\\n[171] S. Latif, Z. Zhou, Y. Kim, F. Beck, N. W. Kim, Kori: Interactive\\nsynthesis of text and charts in data documents, IEEE Transactions on\\nVisualization and Computer Graphics 28 (1) (2021) 184–194.\\n[172] Z. Wang, L. Yuan, L. Wang, B. Jiang, Z. Wei, VirtuWander: Enhanc-\\ning multi-modal interaction for virtual tour guidance through large\\nlanguage models, arXiv preprint arXiv:2401.11923 (2024).\\n[173] T. Zhang, H. Feng, W. Chen, Z. Chen, W. Zheng, X. Luo, W. Huang,\\nA. Tung, Chartnavigator: An interactive pattern identification and\\nannotation framework for charts, IEEE Transactions on Knowledge\\nand Data Engineering 35 (2) (2023) 1258–1269.\\n67'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 67, 'page_label': '68'}, page_content='[174] B. Saleh, M. Dontcheva, A. Hertzmann, Z. Liu, Learning style similar-\\nity for searching infographics (2015). arXiv:1505.01214.\\n[175] Y. Ye, R. Huang, W. Zeng, VISAtlas: An image-based exploration and\\nquery system for large visualization collections via neural image em-\\nbedding, IEEE Transactions on Visualization and Computer Graphics\\n(2022) 1–15.\\n[176] A. Fan, Y. Ma, M. Mancenido, R. Maciejewski, Annotating line charts\\nfor addressing deception, in: Proceedings of the 2022 CHI Conference\\non Human Factors in Computing Systems, Association for Computing\\nMachinery, 2022.\\n[177] K. Davila, S. Setlur, D. S. Doermann, B. U. Kota, V. Govindaraju,\\nChart mining: A survey of methods for automated chart analysis, IEEE\\nTranssactions on Pattern Analysis and Machine Intelligence 43 (11)\\n(2021) 3799–3819.\\n[178] D. J. L. Lee, J. Lee, T. Siddiqui, J. Kim, K. Karahalios, A. G.\\nParameswaran, You can’t always sketch what you want: Understanding\\nsensemaking in visual query systems, IEEE Transactions on Visualiza-\\ntion and Computer Graphics 26 (1) (2020) 1267–1277.\\n[179] S. Cohen, C. Li, J. Yang, C. Yu, Computational journalism: A call to\\narms to database researchers, in: Fifth Biennial Conference on Innova-\\ntive Data Systems Research, CIDR 2011, Asilomar, CA, USA, January\\n9-12, 2011, Online Proceedings, www.cidrdb.org, 2011, pp. 148–151.\\n[180] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin\\ntransformer: Hierarchical vision transformer using shifted windows, in:\\nProceedings of the IEEE/CVF International Conference on Computer\\nVision, 2021, pp. 10012–10022.\\n[181] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transfer-\\nable visual models from natural language supervision, in: International\\nConference on Machine Learning, PMLR, 2021, pp. 8748–8763.\\n[182] F.-Y. Sun, J. Hoffman, V. Verma, J. Tang, InfoGraph: Unsupervised\\nand semi-supervised graph-level representation learning via mutual\\n68'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 68, 'page_label': '69'}, page_content='information maximization, in: International Conference on Learning\\nRepresentations, 2019.\\n[183] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\\nH. K¨ uttler, M. Lewis, W.-t. Yih, T. Rockt¨ aschel, et al., Retrieval-\\naugmented generation for knowledge-intensive nlp tasks, in: Proceed-\\nings of the International Conference on Neural Information Processing\\nSystems, Vol. 33, 2020, pp. 9459–9474.\\n[184] J. Liu, J. Jin, Z. Wang, J. Cheng, Z. Dou, J.-R. Wen, RETA-LLM:\\nA retrieval-augmented large language model toolkit, arXiv preprint\\narXiv:2306.05212 (2023).\\n[185] A. Baldrati, M. Bertini, T. Uricchio, A. Del Bimbo, Effective condi-\\ntioned and composed image retrieval combining clip-based features, in:\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2022, pp. 21466–21474.\\n[186] Y. Ye, Q. Zhu, S. Xiao, K. Zhang, W. Zeng, The contemporary art\\nof image search: Iterative user intent expansion via vision-language\\nmodel, arXiv preprint arXiv:2312.01656 (2023).\\n[187] X. Zeng, Z. Gao, Y. Ye, W. Zeng, IntentTuner: An interactive frame-\\nwork for integrating human intents in fine-tuning text-to-image gener-\\native models, arXiv preprint arXiv:2401.15559 (2024).\\n[188] S. E. Kahou, V. Michalski, A. Atkinson, ´A. K´ ad´ ar, A. Trischler, Y. Ben-\\ngio, FigureQA: An annotated figure dataset for visual reasoning, arXiv\\npreprint arXiv:1710.07300 (2017).\\n[189] J. Zou, G. Wu, T. Xue, Q. Wu, An affinity-driven relation network\\nfor figure question answering, in: IEEE International Conference on\\nMultimedia and Expo, IEEE, 2020, pp. 1–6.\\n[190] D. H. Kim, E. Hoque, M. Agrawala, Answering questions about charts\\nand generating visual explanations, in: Proceedings of CHI Conference\\non Human Factors in Computing Systems, 2020, pp. 1–13.\\n[191] S. Song, J. Chen, C. Li, C. Wang, GVQA: Learning to answer questions\\nabout graphs with visualizations via knowledge base, in: Proceedings\\nof the CHI Conference on Human Factors in Computing Systems, 2023.\\n69'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 69, 'page_label': '70'}, page_content='[192] E. Hoque, P. Kavehzadeh, A. Masry, Chart question answering: State\\nof the art and future directions, Computer Graphics Forum 41 (3)\\n(2022) 555–572.\\n[193] Z. Yang, X. He, J. Gao, L. Deng, A. Smola, Stacked attention networks\\nfor image question answering, in: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2016, pp. 21–29.\\n[194] J. Luo, Z. Li, J. Wang, C.-Y. Lin, ChartOCR: Data extraction from\\ncharts images via a deep hybrid framework, in: Proceedings of the\\nIEEE/CVF Winter Conference on Applications of Computer Vision,\\n2021, pp. 1917–1925.\\n[195] G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun,\\nD. Han, S. Park, OCR-Free document understanding transformer, in:\\nEuropean Conference on Computer Vision, Springer Nature Switzer-\\nland, Cham, 2022, pp. 498–517.\\n[196] J. Cho, J. Lei, H. Tan, M. Bansal, Unifying vision-and-language tasks\\nvia text generation, in: International Conference on Machine Learning,\\nPMLR, 2021, pp. 1931–1942.\\n[197] J. Herzig, P. K. Nowak, T. Mueller, F. Piccinno, J. Eisenschlos, TaPas:\\nWeakly supervised table parsing via pre-training, in: Proceedings of\\nthe Annual Meeting of the Association for Computational Linguistics,\\n2020, pp. 4320–4333.\\n[198] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018).\\n[199] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020).\\n[200] S. Li, N. Tajbakhsh, SciGraphQA: A large-scale synthetic multi-\\nturn question-answering dataset for scientific graphs, arXiv preprint\\narXiv:2308.03349 (2023).\\n70'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 70, 'page_label': '71'}, page_content='[201] C. Andrews, A. Endert, B. Yost, C. North, Information visualization on\\nlarge, high-resolution displays: Issues, challenges, and opportunities,\\nInformation Visualization 10 (4) (2011) 341–355.\\n[202] J. Walny, B. Lee, P. Johns, N. H. Riche, S. Carpendale, Understanding\\npen and touch interaction for data exploration on interactive white-\\nboards, IEEE Transactions on Visualization and Computer Graphics\\n18 (12) (2012) 2779–2788.\\n[203] S. K. Badam, F. Amini, N. Elmqvist, P. Irani, Supporting visual ex-\\nploration for multiple users in large display environments, in: IEEE\\nConference on Visual Analytics Science and Technology, IEEE, 2016,\\npp. 1–10.\\n[204] E. Hoque, V. Setlur, M. Tory, I. Dykeman, Applying pragmatics prin-\\nciples for interaction with visual analytics, IEEE Transactions on Vi-\\nsualization and Computer Graphics 24 (1) (2017) 309–318.\\n[205] A. Srinivasan, B. Lee, N. Henry Riche, S. M. Drucker, K. Hinckley,\\nInChorus: Designing consistent multimodal interactions for data visu-\\nalization on tablet devices, in: Proceedings of the CHI Conference on\\nHuman Factors in Computing Systems, 2020, pp. 1–13.\\n[206] J. Tang, Y. Luo, M. Ouzzani, G. Li, H. Chen, Sevi: Speech-to-\\nvisualization through neural machine translation, in: SIGMOD Con-\\nference, ACM, 2022, pp. 2353–2356.\\n[207] Y. Lin, H. Li, L. Yang, A. Wu, H. Qu, InkSight: Leveraging sketch\\ninteraction for documenting chart findings in computational notebooks,\\narXiv preprint arXiv:2307.07922 (2023).\\n[208] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segment anything,\\narXiv preprint arXiv:2304.02643 (2023).\\n[209] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su,\\nJ. Zhu, et al., Grounding dino: Marrying dino with grounded pre-\\ntraining for open-set object detection, arXiv preprint arXiv:2303.05499\\n(2023).\\n71'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 71, 'page_label': '72'}, page_content='[210] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv\\npreprint arXiv:2304.08485 (2023).\\n[211] S. Liu, H. Miao, Z. Li, M. Olson, V. Pascucci, P.-T. Bremer, AVA:\\nTowards autonomous visualization agents through visual perception-\\ndriven decision-making, arXiv preprint arXiv:2312.04494 (2023).\\n[212] J. Lu, B. Pan, J. Chen, Y. Feng, J. Hu, Y. Peng, W. Chen, AgentLens:\\nVisual analysis for agent behaviors in llm-based autonomous systems,\\narXiv preprint arXiv:2402.08995 (2024).\\n[213] P. Maddigan, T. Susnjak, Chat2vis: Fine-tuning data visualisations\\nusing multilingual natural language text and pre-trained large language\\nmodels (2023). arXiv:2303.14292.\\n[214] R. Yen, J. Zhu, S. Suh, H. Xia, J. Zhao, Coladder: Supporting pro-\\ngrammers with hierarchical code generation in multi-level abstraction,\\narXiv preprint arXiv:2310.08699 (2023).\\n[215] A. Agrawal, I. Kajic, E. Bugliarello, E. Davoodi, A. Gergely, P. Blun-\\nsom, A. Nematzadeh, Reassessing evaluation practices in visual ques-\\ntion answering: A case study on out-of-distribution generalization,\\nin: Findings of the Association for Computational Linguistics: EACL\\n2023, 2023, pp. 1171–1196.\\n[216] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,\\nA. Madotto, P. Fung, Survey of hallucination in natural language gen-\\neration, ACM Computing Surveys 55 (12) (2023) 1–38.\\n[217] K. Hu, S. Gaikwad, M. Hulsebos, M. A. Bakker, E. Zgraggen, C. Hi-\\ndalgo, T. Kraska, G. Li, A. Satyanarayan, C ¸ . Demiralp, VizNet: To-\\nwards a large-scale visualization learning and benchmarking repository,\\nin: Proceedings of the CHI Conference on Human Factors in Comput-\\ning Systems, 2019, pp. 1–12.\\n[218] X. Chen, W. Zeng, Y. Lin, H. M. Ai-Maneea, J. Roberts, R. Chang,\\nComposition and configuration patterns in multiple-view visualiza-\\ntions, IEEE Transactions on Visualization and Computer Graphics\\n27 (2) (2020) 1514–1524.\\n72'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 72, 'page_label': '73'}, page_content='[219] L. Battle, P. Duan, Z. Miranda, D. Mukusheva, R. Chang, M. Stone-\\nbraker, Beagle: Automated extraction and interpretation of visualiza-\\ntions from the web, in: Proceedings of the CHI Conference on Human\\nFactors in Computing Systems, 2018, pp. 1–8.\\n[220] Y. Ye, R. Huang, W. Zeng, VISAtlas: An image-based exploration and\\nquery system for large visualization collections via neural image em-\\nbedding, IEEE Transactions on Visualization and Computer Graphics\\n(2022) 1–15.\\n[221] Y. Luo, J. Tang, G. Li, nvbench: A large-scale synthesized dataset\\nfor cross-domain natural language to visualization task, arXiv preprint\\narXiv:2112.12926 (2021).\\n[222] J. Wang, S. Liu, W. Zhang, Visual analytics for machine learning: A\\ndata perspective survey, arXiv preprint arXiv:2307.07712 (2023).\\n[223] A. I. Anik, A. Bunt, Data-centric explanations: explaining training\\ndata of machine learning systems to promote transparency, in: Pro-\\nceedings of the CHI Conference on Human Factors in Computing Sys-\\ntems, 2021, pp. 1–13.\\n[224] P. Galanter, Generative art theory, A Companion to Digital Art (2016)\\n146–180.\\n[225] H. Wang, P. K. A. Vasu, F. Faghri, R. Vemulapalli, M. Farajtabar,\\nS. Mehta, M. Rastegari, O. Tuzel, H. Pouransari, SAM-CLIP: Merging\\nvision foundation models towards semantic and spatial understanding,\\narXiv preprint arXiv:2310.15308 (2023).\\n[226] Y. Yuan, W. Li, J. Liu, D. Tang, X. Luo, C. Qin, L. Zhang, J. Zhu,\\nOsprey: Pixel understanding with visual instruction tuning, arXiv\\npreprint arXiv:2312.10032 (2023).\\n[227] L. Zhang, A. Rao, M. Agrawala, Adding conditional control to text-\\nto-image diffusion models, in: Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision, 2023, pp. 3836–3847.\\n[228] G. Zheng, X. Zhou, X. Li, Z. Qi, Y. Shan, X. Li, LayoutDiffusion: Con-\\ntrollable diffusion model for layout-to-image generation, in: Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2023, pp. 22490–22499.\\n73'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:41:45+00:00', 'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'keywords': '', 'moddate': '2024-04-30T00:41:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'trapped': '/False', 'source': '/content/2404.18144v1.pdf', 'total_pages': 74, 'page': 73, 'page_label': '74'}, page_content='[229] C. Liu, Y. Guo, X. Yuan, AutoTitle: An interactive title generator\\nfor visualizations, IEEE Transactions on Visualization and Computer\\nGraphics (2023) 1–12.\\n74')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=20)\n",
        "docs=text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "F0rqWrD-87nt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "import re\n",
        "\n",
        "def clean_keys(metadata):\n",
        "    \"\"\"Sanitize metadata keys to make them Weaviate-compatible.\"\"\"\n",
        "    cleaned_metadata = {}\n",
        "    for key, value in metadata.items():\n",
        "        new_key = re.sub(r\"[^A-Za-z0-9_]\", \"_\", key)\n",
        "\n",
        "        if not re.match(r\"^[A-Za-z_]\", new_key):\n",
        "            new_key = \"_\" + new_key\n",
        "\n",
        "        new_key = new_key[:230]\n",
        "\n",
        "        cleaned_metadata[new_key] = value\n",
        "    return cleaned_metadata\n",
        "\n",
        "cleaned_docs = [\n",
        "    Document(page_content=doc.page_content, metadata=clean_keys(doc.metadata))\n",
        "    for doc in docs\n",
        "]\n",
        "\n",
        "vector_db = Weaviate.from_documents(\n",
        "    cleaned_docs, embeddings, client=client, by_text=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "TT87g0M7Mkn_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"What is rag?\",k=3)[0].page_content)"
      ],
      "metadata": {
        "id": "y1SHwFiONZag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a813a19-b25b-4c87-8399-0423ed5d4051"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "items in rows and multi-dimensional attributes in columns. Surrogate\n",
            "data generation studies mainly focus on tabular relational data.\n",
            "Method. A common method for tabular data generation is GAN.\n",
            "• Generative Adversarial Network (GAN) . For example, in recent years,\n",
            "some researchers attempt to generate relational data similar to real\n",
            "data with GANs [63, 38, 65, 66]. The architecture of GANs consists\n",
            "of a generator and a discriminator. The adversarial training scheme\n",
            "where the generator progressively learn to generate more realistic data\n",
            "that can deceive the discriminator enables GANs to model the dis-\n",
            "tribution of real data. For example, table-GAN [38] builds upon the\n",
            "basic deep convolutional GAN (DCGAN) [114] framework and tailor\n",
            "the generation to tabular data. Specifically, first the tabular records are\n",
            "converted into square matrix to accommodate convolution operation.\n",
            "In addition to the original generator and discriminator, table-GAN also\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\"What is rag?\",k=3)[1].page_content\n",
        ")"
      ],
      "metadata": {
        "id": "rbUAn-ADOBKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd850e76-9a74-4c0c-aca6-0d70dc98d1e1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N. Schiefer, Z. Hatfield-Dodds, N. DasSarma, E. Tran-Johnson, et al.,\n",
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\"What is rag?\",k=3)[2].page_content\n",
        ")"
      ],
      "metadata": {
        "id": "kPYaJ9FlOdqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e29ab7-9292-4e77-b05b-798c9fee2780"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62] W. Yang, M. Liu, Z. Wang, S. Liu, Foundation models meet visual-\n",
            "izations: Challenges and opportunities, Computational Visual Media\n",
            "(2023).\n",
            "[63] J. Fan, T. Liu, G. Li, J. Chen, Y. Shen, X. Du, Relational data synthe-\n",
            "sis using generative adversarial networks: A design space exploration,\n",
            "Proceedings of the VLDB Endowment 13 (11) (2020) 1962–1975.\n",
            "[64] H. Chen, S. Jajodia, J. Liu, N. Park, V. Sokolov, V. Subrahmanian,\n",
            "FakeTables: Using gans to generate functional dependency preserving\n",
            "tables with bounded real data., in: Proceedings of IJCAI, 2019, pp.\n",
            "2074–2080.\n",
            "[65] X. Qinl, C. Chai, N. Tang, J. Li, Y. Luo, G. Li, Y. Zhu, Synthesizing\n",
            "privacy preserving entity resolution datasets, in: Proceedings of IEEE\n",
            "International Conference on Data Engineering, 2022, pp. 2359–2371.\n",
            "[66] L. Xu, M. Skoularidou, A. Cuesta-Infante, K. Veeramachaneni, Model-\n",
            "ing tabular data using conditional gan, in: Proceedings of the Interna-\n",
            "tional Conference on Neural Information Processing Systems, Vol. 32,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"What is attention?\",k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5dtX3baOmiQ",
        "outputId": "3c28bf05-e27a-4369-bc80-9a4685d4fe0e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 62, 'page_label': '63', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='[126] M. A. Borkin, A. A. Vo, Z. Bylinskii, P. Isola, S. Sunkavalli, A. Oliva,\\nH. Pfister, What makes a visualization memorable?, IEEE Transactions\\non Visualization and Computer Graphics 19 (12) (2013) 2306–2315.\\n[127] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, M. Jagersand, BAS-\\nNet: Boundary-aware salient object detection, in: Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2019, pp. 7479–7489.\\n[128] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely con-\\nnected convolutional networks, in: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2017, pp. 4700–4708.\\n[129] L. Dinh, D. Krueger, Y. Bengio, Nice: Non-linear independent compo-\\nnents estimation, arXiv preprint arXiv:1410.8516 (2014).\\n[130] A. Narechania, A. Srinivasan, J. Stasko, NL4DV: A toolkit for generat-\\ning analytic specifications for data visualization from natural language\\nqueries, IEEE Transactions on Visualization and Computer Graphics'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 30, 'page_label': '31', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='reference sources for style transfer, taping into the inherent visual appeal\\nand cognitive stimulation provided by natural images. Recent research has\\nshown that natural images can also serve as an adorable source to stimulate\\nhuman intelligence [92, 156].\\n31'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 68, 'page_label': '69', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='of the CHI Conference on Human Factors in Computing Systems, 2023.\\n69')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lbQYIphuO7K4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "h6JaVazormlu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4JiOeNkrqw8",
        "outputId": "2cf076a1-da84-41db-d843-fe3983b17e49"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n"
      ],
      "metadata": {
        "id": "PCv820Ptrrq4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingfacehub_api_token=userdata.get('HUGGINGFACE_TOKEN')"
      ],
      "metadata": {
        "id": "2mhSIBx1rvRB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\":180}\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x_mVFtYr4jt",
        "outputId": "d0030955-bc6b-44d1-d457-6637e60ad0f4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-cc8e38a048ba>:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  model = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n"
      ],
      "metadata": {
        "id": "_CnVGlqgr9yg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser=StrOutputParser()\n"
      ],
      "metadata": {
        "id": "huZHfzXpsBM-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vector_db.as_retriever()\n"
      ],
      "metadata": {
        "id": "v2WQ-lTisEbU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "ZF664wWRsGN6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"what is Disentangled Representation Learning \"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRMKX3WOsIwm",
        "outputId": "298db1f6-7433-4dc2-b28d-6ff66ba454cf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: what is Disentangled Representation Learning \n",
            "Context: [Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 14, 'page_label': '15', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='Concept\\nImage\\nVAE Disentangling\\nDisentangle\\nUnseen Cases\\nTraining Data\\n(a) VATLD (b) SCANViz\\nSemantic Adversarial Learning\\nFigure 3: Data enhancement with disentangled representation learning, such as\\nVATLD [48] and SCANViz [73].\\ncomputer graphics, disentanglement has been an essential technique for\\ncontrollable generation [121, 122, 123]. A commonly used DRL archi-\\ntecture is β-VAE [124]. The objective function of β-VAE is a modifica-\\ntion of the original VAE with an additional β parameter. Experiments\\nshow that better chosen β value (typically < 1) can produce more\\ndisentangled latent representation z. For example, VATLD [48] is a vi-\\nsual analytics system that adapts β-VAE to extract user-interpretable\\nfeatures like colors, background and rotation from low-level features\\nof traffic light images. With such interpretable features encoded in\\nlatent space, users can generate additional training examples in an in-\\nterpretable manner to enhance the traffic light detection model. The'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 61, 'page_label': '62', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='[122] I. Jeon, W. Lee, M. Pyeon, G. Kim, Ib-GAN: Disentangled represen-\\ntation learning with information bottleneck generative adversarial net-\\nworks, in: Proceedings of the AAAI Conference on Artificial Intelli-\\ngence, Vol. 35, 2021, pp. 7926–7934.\\n[123] C. Zhou, F. Zhong, C. ¨Oztireli, CLIP-PAE: Projection-augmentation\\nembedding to extract relevant features for a disentangled, interpretable\\nand controllable text-guided face manipulation, in: Proceedings of the\\nACM SIGGRAPH Conference, 2023, pp. 1–9.\\n[124] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Des-\\njardins, A. Lerchner, Understanding disentangling in beta-vae, arXiv\\npreprint arXiv:1804.03599 (2018).\\n[125] J. Chen, M. Ling, R. Li, P. Isenberg, T. Isenberg, M. Sedlmair,\\nT. M¨ oller, R. S. Laramee, H.-W. Shen, K. W¨ unsche, et al., Vis30k: A\\ncollection of figures and tables from ieee visualization conference pub-\\nlications, IEEE Transactions on Visualization and Computer Graphics\\n27 (9) (2021) 3826–3833.\\n62'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 13, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='tract spatial features. Then features of adjacent time steps are fed into\\nConvLSTM to evaluate temporal coherence. Global average pooling is\\nused to produce the final single value score for realness.\\n• Disentangled Representation Learning (DRL). DRL with VAEs or GANs\\nhave been applied to visual analytics to identify interpretable dimen-\\nsions interactively [48, 73, 69, 72, 74]. The disentangled dimensions\\ncan be subsequently controlled by users to generate meaningful data\\nfor augmentation. In the general literature of computer vision and\\n14'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 61, 'page_label': '62', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='[117] J. Choi, T. Kim, C. Kim, Self-ensembling with gan-based data augmen-\\ntation for domain adaptation in semantic segmentation, in: Proceed-\\nings of the IEEE/CVF International Conference on Computer Vision,\\n2019, pp. 6830–6840.\\n[118] X. Liu, Y. Zou, L. Kong, Z. Diao, J. Yan, J. Wang, S. Li, P. Jia,\\nJ. You, Data augmentation via latent space interpolation for image\\nclassification, in: Proceedings of the International Conference on Pat-\\ntern Recognition (ICPR), 2018, pp. 728–733.\\n[119] D. P. Kingma, M. Welling, Auto-encoding variational bayes, arXiv\\npreprint arXiv:1312.6114 (2013).\\n[120] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial networks,\\nCommunications of the ACM 63 (11) (2020) 139–144.\\n[121] L. Tran, X. Yin, X. Liu, Disentangled representation learning gan for\\npose-invariant face recognition, in: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2017, pp. 1415–1424.')]\n",
            "Answer:\n",
            "Disentangled Representation Learning (DRL) is a technique used in visual analytics to identify interpretable dimensions interactively. It involves using Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs) to learn representations where each dimension corresponds to a specific feature or factor of variation in the data. This allows users to control and generate meaningful data for augmentation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"How does the RAG model differ from traditional language generation models?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R15ejBM0sN9m",
        "outputId": "446922f8-c847-4e8f-e9e3-752ee4601552"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: How does the RAG model differ from traditional language generation models?\n",
            "Context: [Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 63, 'page_label': '64', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='Language models (mostly) know what they know, arXiv preprint\\narXiv:2207.05221 (2022).\\n[136] S. Hegselmann, A. Buendia, H. Lang, M. Agrawal, X. Jiang, D. Son-\\ntag, TabLLM: Few-shot classification of tabular data with large lan-\\nguage models, in: International Conference on Artificial Intelligence\\nand Statistics, PMLR, 2023, pp. 5549–5581.\\n[137] Y. Luo, X. Qin, C. Chai, N. Tang, G. Li, W. Li, Steerable self-driving\\ndata visualization, IEEE Transactions on Knowledge and Data Engi-\\nneering 34 (1) (2020) 475–490.\\n[138] X. Qin, Y. Luo, N. Tang, G. Li, DeepEye: An automatic big data\\nvisualization framework, Big Data Mining and Analytics 1 (1) (2018)\\n75–82.\\n[139] X. Qin, Y. Luo, N. Tang, G. Li, Deepeye: Visualizing your data by\\nkeyword search, in: EDBT, OpenProceedings.org, 2018, pp. 441–444.\\n[140] A. Voynov, K. Aberman, D. Cohen-Or, Sketch-guided text-to-image\\ndiffusion models, in: ACM SIGGRAPH Conference Proceedings, 2023,\\npp. 1–11.'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 45, 'page_label': '46', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='their own dataset or utilize the datasets created by prior works [23]. Even\\nin the era of large language models which are pre-trained on much larger\\ngeneral purpose dataset, a domain-specific visualization dataset can serve as\\nvaluable reference and knowledge base for efficient prompting and improving\\nthe reliability of GenAI results. The quality, quantity, and diversity of the\\ndataset thus have a significant impact on the generative performance and\\nthe output quality, as it determines how the GenAI model perceives and\\nunderstands the patterns and semantics of the generation requirement and\\ngenerated content.\\nIn this regard, several aspects warrant special attention in future research.\\nFirst, the diversity is important, as a diverse training dataset helps the AI\\nmodel learn a broader range of topics, styles and other design patterns in real-\\nworld visualization. This diversity enables the model to generate content that'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 3, 'page_label': '4', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='Sequence generation models, like LSTMs and Transformers, can be\\nused to create content with a sequential or temporal structure.\\n• Tabular Generation: This category covers the generation of struc-\\ntured data in the form of rows and columns, such as spreadsheets or\\ndatabase tables. Applications include data augmentation, anonymiza-\\ntion, and data imputation.\\n4'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 48, 'page_label': '49', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='trolNet [227] and LayoutDiffusion [228] to embed the control into the model\\nitself. Alternatively, researchers can develop interactive tools to support hu-\\nman intervention in the generation process [229].\\n8. Conclusion\\nThe burgeoning GenAI technology is promising for applications in the\\nvisualization domain. Because of GenAI’s impressive capacity to model the\\ntransformation and design process by learning from real data, it can ben-\\nefit a range of visualization tasks like data enhancement, visual mapping\\ngeneration, stylization and interaction. Different types of GenAI methods\\nhave been applied to these tasks due to different data structures, including\\nsequence generation, tabular generation, spatial generation and graph gener-\\nation. With the advent of latest GenAI technology like large language model\\nand diffusion model, new opportunities emerge to revolutionize GenAI4VIS\\nmethods. However, task-specific challenges still exist due to the unique')]\n",
            "Answer:\n",
            "The RAG (Retrieval-Augmented Generation) model differs from traditional language generation models in that it uses external knowledge sources to augment its generation capabilities. Traditional models generate responses based solely on their internal parameters, while RAG models retrieve relevant information from external sources to enhance the generated output. This allows RAG models to provide more accurate and up-to-date information, as they can access a broader range of data than what they were initially trained on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"What is Sequence Generation:?\"))"
      ],
      "metadata": {
        "id": "v7Oph24hsRV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42db95cd-f8ad-4ec9-881f-c691d8c0d2cb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: What is Sequence Generation:?\n",
            "Context: [Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 3, 'page_label': '4', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='Sequence generation models, like LSTMs and Transformers, can be\\nused to create content with a sequential or temporal structure.\\n• Tabular Generation: This category covers the generation of struc-\\ntured data in the form of rows and columns, such as spreadsheets or\\ndatabase tables. Applications include data augmentation, anonymiza-\\ntion, and data imputation.\\n4'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 3, 'page_label': '4', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='utilizing existing media such as text, graphics, audio, and video [7, 8]. A\\nkey feature of GenAI is that it generates new content by learning from data\\ninstead of explicit programs.\\nGenAI methods categorization. Despite the differences between differ-\\nent domain targets of generation ranging from text, code, multi-media to 3D\\ngeneration, the particular algorithms of generation actually depend on the\\ndata structures which show common characteristics across different domains.\\nParticularly, in GenAI4VIS applications, categorization based on data struc-\\ntures can facilitate more concrete understanding of the algorithms in relation\\nto the different types of data involved in different visualization tasks. Here,\\nwe provide an overview of different types of GenAI in terms of typical data\\nstructures associated with data visualization.\\n• Sequence Generation: This category includes the generation of or-\\ndered data, such as text, code, music, videos, and time-series data.'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 47, 'page_label': '48', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='rant of structural information in the previous sequence generation. Moreover,\\nsome recent studies in AI show that merging two large pretrained models us-\\ning techniques like knowledge distillation can not only produce a versatile\\nmerged model but also boost the performance for downstream tasks that\\nrequire knowledge from both models [225], which provides inspiration for\\npotential strategies to improve integration of GenAI4VIS models.\\nIn fact, the gap between visualization and GenAI pipelines is not neces-\\nsarily a downside, as this signifies opportunities for future research to com-\\nbine the advantages while mitigating the respective disadvantages. On the\\none hand, visualization researchers can think about how to directly model\\nthe mapping between data and views in the end-to-end statistical learning\\nframework of GenAI, which can provide more effective learning and eval-\\nuation based on the final visual representations. For example, this means'), Document(metadata={'author': 'Yilin Ye; Jianing Hao; Yihan Hou; Zhan Wang; Shishi Xiao; Yuyu Luo; Wei Zeng;', 'creationdate': '2024-04-30T00:41:45Z', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-30T00:41:45Z', 'page': 6, 'page_label': '7', 'producer': 'pdfTeX-1.40.25', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2404.18144v1.pdf', 'subject': '', 'title': 'Generative AI for Visualization: State of the Art and Future Directions', 'total_pages': 74, 'trapped': '/False'}, page_content='many applications such as colormap generation [28]. Some studies also lever-\\nage optimization-based methods to minimize expert-defined explicit objective\\nfunctions. However, these types of methods differ from GenAI methods in\\nthat they are top-down and do not learn from real-world data. To narrow\\ndown the scope of our survey, we exclude all previous generative algorithms\\nthat are purely based on rules or optimization.\\nRelation between different GenAI methods and tasks. Due to the\\nwide range of diverse applications in GenAI4VIS, there is no clear-cut one-\\nto-one relation between the type of GenAI methods and the tasks. Neverthe-\\nless, we can observe some interesting correlation. First, sequence generation\\nis mostly applied in visual mapping or interaction-related tasks. This is\\nbecause GenAI such as translation models and the latest LLMs or vision-\\nlanguage model are useful in generating sequence of code specifying visual')]\n",
            "Answer:\n",
            "Sequence Generation involves creating content with a sequential or temporal structure, such as text, code, music, videos, and time-series data. It uses models like LSTMs and Transformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9heu-j0QdtQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}